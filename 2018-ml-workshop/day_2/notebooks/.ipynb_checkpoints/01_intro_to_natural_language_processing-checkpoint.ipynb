{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An Introduction to Natural Language Processing (NLP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is NLP?\n",
    "\n",
    "Natural language processing (NLP) is the set of techniques developed to automatically process/analyze/understand/ generate natural human languages.\n",
    "\n",
    "### Why is this useful?\n",
    "\n",
    "Most of the knowledge that has accrued over the course of human history is stored as unstructured text and we need some way to make sense out of all of it.\n",
    "\n",
    "**Simply put, NLP enables the automatic, quantitative analysis of this unstructured text.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common NLP subproblems\n",
    "\n",
    "- **Speech recognition and generation**: [Apple siri](https://www.apple.com/ios/siri/)\n",
    "    - Speech to text\n",
    "    - Text to speech\n",
    "- **Question answering**: [IBM Watson](https://www.ibm.com/watson/)\n",
    "    - Match query with knowledge base\n",
    "    - Reasoning about intent of question\n",
    "- **Machine translation**: [Google Translate](https://translate.google.com/)\n",
    "    - One language to another to another\n",
    "- **Information retrieval**: [Google](https://www.google.com/)\n",
    "    - Finding relevant results\n",
    "    - Finding similar results\n",
    "- **Information extraction**: [Gmail](https://www.google.com/gmail/)\n",
    "    - Structured information from unstructured documents\n",
    "- **Assistive technologies**: Google autocompletion\n",
    "    - Predictive text input\n",
    "    - Text simplification\n",
    "- **Natural Language Generation**: [Narrative Science](https://narrativescience.com/)\n",
    "    - Generating text from data\n",
    "- **Automatic summarization**: [Google News](https://news.google.com/news/?ned=us&gl=US&hl=en)\n",
    "    - Extractive summarization\n",
    "    - Abstractive summarization\n",
    "- [**Sentiment analysis**](https://en.wikipedia.org/wiki/Sentiment_analysis):\n",
    "    - Attitude of speaker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are some of the lower level components?\n",
    "\n",
    "- **Tokenization**: breaking text into tokens (words, sentences, n-grams)\n",
    "- **Stopword removal**: \n",
    "    - repetitive & redundant gap-filling utterances (e.g., \"like\") \n",
    "    - bridge words (e.g., a/an/the)\n",
    "- **Stemming and lemmatization**: root word\n",
    "- **TF-IDF**: word importance\n",
    "- **Part-of-speech tagging**: noun/verb/adjective\n",
    "- **Named entity recognition**: person/organization/location\n",
    "- **Spelling correction**: \"New Yrok City\"\n",
    "- **Word sense disambiguation**: \"buy a mouse\"\n",
    "- **Segmentation**: \"New York City subway\"\n",
    "- **Language detection**: \"translate this page\"\n",
    "- **Machine learning**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why is NLP hard?\n",
    "\n",
    "- **Ambiguity**:\n",
    "    - Teacher Strikes Idle Kids\n",
    "    - Red Tape Holds Up New Bridges\n",
    "    - Hospitals are Sued by 7 Foot Doctors\n",
    "    - Juvenile Court to Try Shooting Defendant\n",
    "    - Local High School Dropouts Cut in Half\n",
    "- **Non-standard English**: tweets/text messages\n",
    "- **Idioms**: \"throw in the towel\"\n",
    "- **Newly coined words**: \"retweet\"\n",
    "- **Tricky entity names**: \"Where is A Bug's Life playing?\"\n",
    "- **World knowledge**: \"Mary and Sue are sisters\", \"Mary and Sue are mothers\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How does NLP work?\n",
    "\n",
    "- Build probabilistic model using data about a language\n",
    "- Requires an understanding of the language\n",
    "- Requires an understanding of the world (or a particular domain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading in a subset of SemEval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dependencies #\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "\n",
    "# for unbalanced data \n",
    "from sklearn.model_selection import train_test_split, StratifiedShuffleSplit\n",
    "\n",
    "# vectorizer (this one ignores the syntax -i.e., order and focuses on frequency)\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "# \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the reviews into a DataFrame\n",
    "semeval_data = pd.read_csv(\"../data/semeval_sampled_cleaned_data.csv\",sep=\"\\t\",names=[\"id\",\"sentiment\",\"tweet\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7105, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>633616008910082048</td>\n",
       "      <td>negative</td>\n",
       "      <td>Donald Trump and Scott Walker would Negros bac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>639974060466663424</td>\n",
       "      <td>negative</td>\n",
       "      <td>@YidVids2 probably not cause he played with th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>664049298624114688</td>\n",
       "      <td>negative</td>\n",
       "      <td>Woaw just because briana is \"having\" louis' ba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>665627471899975680</td>\n",
       "      <td>negative</td>\n",
       "      <td>I wrote this about the 'SAS response' after th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>522951269271339008</td>\n",
       "      <td>negative</td>\n",
       "      <td>@MasterDebator_ @NFLosophy 2nd best in luck dr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>109016744177319937</td>\n",
       "      <td>negative</td>\n",
       "      <td>Boehner tells Obama that sorry, the House is a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>641475286228271108</td>\n",
       "      <td>negative</td>\n",
       "      <td>I think Google may be worried that if they all...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>636734909646815233</td>\n",
       "      <td>negative</td>\n",
       "      <td>This goes right up there with Rolling Stone pu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>636155339067424768</td>\n",
       "      <td>negative</td>\n",
       "      <td>If Carly Fiorina ran the US the way she ran HP...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>111733236627025920</td>\n",
       "      <td>negative</td>\n",
       "      <td>@HartsPub ru showing the All Blacks v Tonga ga...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   id sentiment  \\\n",
       "0  633616008910082048  negative   \n",
       "1  639974060466663424  negative   \n",
       "2  664049298624114688  negative   \n",
       "3  665627471899975680  negative   \n",
       "4  522951269271339008  negative   \n",
       "5  109016744177319937  negative   \n",
       "6  641475286228271108  negative   \n",
       "7  636734909646815233  negative   \n",
       "8  636155339067424768  negative   \n",
       "9  111733236627025920  negative   \n",
       "\n",
       "                                               tweet  \n",
       "0  Donald Trump and Scott Walker would Negros bac...  \n",
       "1  @YidVids2 probably not cause he played with th...  \n",
       "2  Woaw just because briana is \"having\" louis' ba...  \n",
       "3  I wrote this about the 'SAS response' after th...  \n",
       "4  @MasterDebator_ @NFLosophy 2nd best in luck dr...  \n",
       "5  Boehner tells Obama that sorry, the House is a...  \n",
       "6  I think Google may be worried that if they all...  \n",
       "7  This goes right up there with Rolling Stone pu...  \n",
       "8  If Carly Fiorina ran the US the way she ran HP...  \n",
       "9  @HartsPub ru showing the All Blacks v Tonga ga...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(semeval_data.shape)\n",
    "semeval_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7095</th>\n",
       "      <td>252086067849732098</td>\n",
       "      <td>positive</td>\n",
       "      <td>Varitek eager to learn on the job: Newly appoi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7096</th>\n",
       "      <td>639949454028746752</td>\n",
       "      <td>positive</td>\n",
       "      <td>@KrissyLynnxxx Sexxxy booty! Thanks 4 sharing ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7097</th>\n",
       "      <td>630562966690230272</td>\n",
       "      <td>positive</td>\n",
       "      <td>Rest in peace Frank gifford ! my thoughts and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7098</th>\n",
       "      <td>629789335123066880</td>\n",
       "      <td>positive</td>\n",
       "      <td>Looking forward to being at MetLife tomorrow f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7099</th>\n",
       "      <td>640650526548344832</td>\n",
       "      <td>positive</td>\n",
       "      <td>Spent Sunday afternoon upgrading to Windows 10...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7100</th>\n",
       "      <td>637091107222908928</td>\n",
       "      <td>positive</td>\n",
       "      <td>Windows 10 user can stream the Apple event lat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7101</th>\n",
       "      <td>625138186914820096</td>\n",
       "      <td>positive</td>\n",
       "      <td>Hanging with my Monsters on a Saturday night w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7102</th>\n",
       "      <td>627485352148774912</td>\n",
       "      <td>positive</td>\n",
       "      <td>Since 2013 Danny Valencia is the 21st best hit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7103</th>\n",
       "      <td>636310021089923074</td>\n",
       "      <td>positive</td>\n",
       "      <td>CHARLOTTE the DAUGHTER OF RIC FLAIR called the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7104</th>\n",
       "      <td>256076206263390208</td>\n",
       "      <td>positive</td>\n",
       "      <td>Rahul Dravid &amp;lt;3 You may not be in the Natio...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      id sentiment  \\\n",
       "7095  252086067849732098  positive   \n",
       "7096  639949454028746752  positive   \n",
       "7097  630562966690230272  positive   \n",
       "7098  629789335123066880  positive   \n",
       "7099  640650526548344832  positive   \n",
       "7100  637091107222908928  positive   \n",
       "7101  625138186914820096  positive   \n",
       "7102  627485352148774912  positive   \n",
       "7103  636310021089923074  positive   \n",
       "7104  256076206263390208  positive   \n",
       "\n",
       "                                                  tweet  \n",
       "7095  Varitek eager to learn on the job: Newly appoi...  \n",
       "7096  @KrissyLynnxxx Sexxxy booty! Thanks 4 sharing ...  \n",
       "7097  Rest in peace Frank gifford ! my thoughts and ...  \n",
       "7098  Looking forward to being at MetLife tomorrow f...  \n",
       "7099  Spent Sunday afternoon upgrading to Windows 10...  \n",
       "7100  Windows 10 user can stream the Apple event lat...  \n",
       "7101  Hanging with my Monsters on a Saturday night w...  \n",
       "7102  Since 2013 Danny Valencia is the 21st best hit...  \n",
       "7103  CHARLOTTE the DAUGHTER OF RIC FLAIR called the...  \n",
       "7104  Rahul Dravid &lt;3 You may not be in the Natio...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "semeval_data.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "positive    0.511612\n",
       "negative    0.488388\n",
       "Name: sentiment, dtype: float64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# frequency of positive vs. negative tweets\n",
    "semeval_data.sentiment.value_counts()/semeval_data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I wish I was in Bolton tonight :('"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sample & read a random tweet\n",
    "semeval_data.tweet[20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Terminology**\n",
    "- **corpus:** collection of documents \n",
    "    - document: individual row in the data -i.e., a tweet\n",
    "- **corpora:** plural form of corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting tweets to utf (best encoding method up to date)\n",
    "semeval_data.tweet = semeval_data.tweet.map(lambda x: x.encode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting sentiment as 1's and 0's\n",
    "semeval_data[\"target\"] = (semeval_data.sentiment==\"positive\").astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the new DataFrame into training and testing sets, keeping relative frequencies of targets unchanged\n",
    "splitter = StratifiedShuffleSplit(n_splits=1,\n",
    "                                  test_size=0.3)\n",
    "train_indices,test_indices=list(splitter.split(semeval_data.tweet,semeval_data.target))[0]\n",
    "X_train,y_train = semeval_data.tweet.iloc[train_indices],semeval_data.target.iloc[train_indices]\n",
    "X_test,y_test = semeval_data.tweet.iloc[test_indices],semeval_data.target.iloc[test_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    0.511562\n",
       "0    0.488438\n",
       "Name: target, dtype: float64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make sure training dataset matches the frequency of +ve and -ve sentiment frequency\n",
    "y_train.value_counts()/y_train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4973,)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2132,)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    0.511726\n",
       "0    0.488274\n",
       "Name: target, dtype: float64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.value_counts()/y_test.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **What:** Separate text into units such as sentences or words\n",
    "- **Why:** Gives structure to previously unstructured text\n",
    "- **Notes:** Relatively easy with English language text, not easy with some languages\n",
    "- **Tweets:** Traditionally in NLP, you throw out any non alphabetical characters/symbols and numbers. With tweets, certain additional symbols convey meaning (hashtags, emoticons, etc.), so we will need to parse tweets a bit differently than we would a normal piece of text.\n",
    "\n",
    "Here we import a special tweet-specific tokenizer that can handle what is described above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    " \n",
    "emoticons_str = r\"\"\"\n",
    "    (?:\n",
    "        [:=;] # Eyes\n",
    "        [oO\\-]? # Nose (optional)\n",
    "        [D\\)\\]\\(\\]/\\\\OpP] # Mouth\n",
    "    )\"\"\"\n",
    " \n",
    "regex_str = [\n",
    "    emoticons_str,\n",
    "    r'<[^>]+>', # HTML tags\n",
    "    r'(?:@[\\w_]+)', # @-mentions\n",
    "    r\"(?:\\#+[\\w_]+[\\w\\'_\\-]*[\\w_]+)\", # hash-tags\n",
    "    r'http[s]?://(?:[a-z]|[0-9]|[$-_@.&amp;+]|[!*\\(\\),]|(?:%[0-9a-f][0-9a-f]))+', # URLs\n",
    " \n",
    "    r'(?:(?:\\d+,?)+(?:\\.?\\d+)?)', # numbers\n",
    "    r\"(?:[a-z][a-z'\\-_]+[a-z])\", # words with - and '\n",
    "    r'(?:[\\w_]+)', # other words\n",
    "    r'(?:\\S)' # anything else\n",
    "]\n",
    "    \n",
    "tokens_re = re.compile(r'('+'|'.join(regex_str)+')', re.VERBOSE | re.IGNORECASE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer \n",
    "from nltk.tokenize import TweetTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_for_tweets = TweetTokenizer(strip_handles = True, # remove @'s\n",
    "                           preserve_case = False, # otherwise upper and lower-cases are treated as different words\n",
    "                           reduce_len = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **token_pattern:** string\n",
    "- Regular expression denoting what constitutes a \"token\". The default regexp selects tokens of 2 or more alphanumeric characters (punctuation is completely ignored and always treated as a token separator).\n",
    "- **tokenizer:** callable/function\n",
    "- Function that converts a string into a list of tokens using some arbitrary logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use CountVectorizer to create document-term matrices from X_train and X_test\n",
    "## dictionary contains unique tokens extracted from documents\n",
    "vect = CountVectorizer(tokenizer = tokenizer_for_tweets.tokenize)\n",
    "\n",
    "# training data properties must be applied to tested on testing data (e.g., tf-idf)\n",
    "train_dtm = vect.fit_transform(X_train) # counting the tokens\n",
    "test_dtm = vect.transform(X_test) # transforming is also doing the same thing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of tokens in training data: 15703\n",
      "# of tokens in testing data: 15040\n"
     ]
    }
   ],
   "source": [
    "# rows are documents, columns are terms (aka \"tokens\" or \"features\")\n",
    "print(\"# of tokens in training data:\", train_dtm.shape[1])\n",
    "print(\"# of tokens in testing data:\", test_dtm.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['youuu', 'yr', 'yrs', 'yum', 'yup', 'yushin', 'yuvraj', 'yv', 'z', 'z5', 'z90', 'zac', 'zach', 'zachary', 'zack', 'zan', 'zanjeer', 'zarif.all', 'zarry', 'zayn', \"zayn's\", 'zbb', 'zen', 'zero', 'zerohedge', 'zerries', 'zeus', 'zhuo', 'zimmer', 'zimmerman', 'zing', 'zionism', 'zionist', 'zlatan', \"zlatan's\", 'zoe', 'zombie', 'zombies', 'zone', 'zones', 'zoo', 'zowie', 'zubuchon', 'zuck', 'zuckerman', 'zulu', 'zumiez', '|', '|=', '~']\n"
     ]
    }
   ],
   "source": [
    "# last 50 features\n",
    "print(vect.get_feature_names()[-50:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<4973x15040 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 102743 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compressed sparse format for a large dataset\n",
    "train_dtm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=<bound method TweetTokenizer.tokenize of <nltk.tokenize.casual.TweetTokenizer object at 0x1a134644a8>>,\n",
       "        vocabulary=None)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show vectorizer options\n",
    "vect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the [CountVectorizer documentation](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) to get a better understanding of how it works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **lowercase:** `boolean`, `True` by default\n",
    "- Convert all characters to lowercase before tokenizing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4973, 15042)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# don't convert to lowercase\n",
    "vect = CountVectorizer(lowercase = False,\n",
    "                       tokenizer = tokenizer_for_tweets.tokenize)\n",
    "\n",
    "train_dtm = vect.fit_transform(X_train)\n",
    "train_dtm.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the impact of not lowercasing the text?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of tokens for this tokenizer: 15703\n"
     ]
    }
   ],
   "source": [
    "# allow tokens of one character\n",
    "vect = CountVectorizer(token_pattern = r'(?u)\\b\\w+\\b')\n",
    "\n",
    "train_dtm = vect.fit_transform(X_train)\n",
    "\n",
    "# number of tokens \n",
    "print(\"# of tokens for this tokenizer:\", train_dtm.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **ngram_range:** tuple (min_n, max_n)\n",
    "- The lower and upper boundary of the range of n-values for different n-grams to be extracted. All values of n such that min_n <= n <= max_n will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of tokens for this tokenizer: 81131\n"
     ]
    }
   ],
   "source": [
    "# include 1-grams and 2-grams\n",
    "vect = CountVectorizer(ngram_range = (1, 2), # min = 1, max = 2\n",
    "                       tokenizer = tokenizer_for_tweets.tokenize)\n",
    "train_dtm = vect.fit_transform(X_train)\n",
    "\n",
    "print(\"# of tokens for this tokenizer:\", train_dtm.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['youuu', 'yr', 'yrs', 'yum', 'yup', 'yushin', 'yuvraj', 'yv', 'z', 'z5', 'z90', 'zac', 'zach', 'zachary', 'zack', 'zan', 'zanjeer', 'zarif.all', 'zarry', 'zayn', \"zayn's\", 'zbb', 'zen', 'zero', 'zerohedge', 'zerries', 'zeus', 'zhuo', 'zimmer', 'zimmerman', 'zing', 'zionism', 'zionist', 'zlatan', \"zlatan's\", 'zoe', 'zombie', 'zombies', 'zone', 'zones', 'zoo', 'zowie', 'zubuchon', 'zuck', 'zuckerman', 'zulu', 'zumiez', '|', '|=', '~']\n"
     ]
    }
   ],
   "source": [
    "# last 50 features\n",
    "print(vect.get_feature_names()[-50:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now, build a LR model predicting sentiment:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.81\n"
     ]
    }
   ],
   "source": [
    "# use default options for CountVectorizer\n",
    "vect = CountVectorizer(tokenizer = tokenizer.tokenize)\n",
    "\n",
    "# create document-term matrices\n",
    "train_dtm = vect.fit_transform(X_train)\n",
    "test_dtm = vect.transform(X_test)\n",
    "\n",
    "# use Logistic Regression to predict the star rating\n",
    "lr = LogisticRegression()\n",
    "lr.fit(train_dtm, y_train)\n",
    "y_pred_class = lr.predict(test_dtm)\n",
    "\n",
    "# calculate accuracy\n",
    "print(metrics.accuracy_score(y_test, y_pred_class).round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5117260787992496"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate null accuracy\n",
    "y_test.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 15040)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.coef_.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome, so just using raw counts of different words gives us very good performance!\n",
    "\n",
    "Let's examine the most positive and most negative words (their coefficients will tell us whether they are most indicative of a positive or negative review)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 'a'), (2, 'b'), (3, 'c'), (4, 'd')]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# zip objects extentiate the two vectors \n",
    "a = [1, 2, 3, 4]\n",
    "b = [\"a\", \"b\", \"c\", \"d\"]\n",
    "\n",
    "list(zip(a, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_coeffs = pd.DataFrame(list(zip(vect.get_feature_names(), \n",
    "                                       lr.coef_[0])),\n",
    "                              columns=[\"word\",\"coeff\"]) # naming the two columns\n",
    "feature_coeffs = feature_coeffs.sort_values(by=\"coeff\",ascending=False).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Words most indicative of positive review:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>coeff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>best</td>\n",
       "      <td>2.236870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>awesome</td>\n",
       "      <td>2.126289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>:)</td>\n",
       "      <td>1.973052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>amazing</td>\n",
       "      <td>1.842102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fun</td>\n",
       "      <td>1.838183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>love</td>\n",
       "      <td>1.736795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>forget</td>\n",
       "      <td>1.433278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>excited</td>\n",
       "      <td>1.288761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>sting</td>\n",
       "      <td>1.265486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>happy</td>\n",
       "      <td>1.258978</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      word     coeff\n",
       "0     best  2.236870\n",
       "1  awesome  2.126289\n",
       "2       :)  1.973052\n",
       "3  amazing  1.842102\n",
       "4      fun  1.838183\n",
       "5     love  1.736795\n",
       "6   forget  1.433278\n",
       "7  excited  1.288761\n",
       "8    sting  1.265486\n",
       "9    happy  1.258978"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# most positive words\n",
    "feature_coeffs.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Words most indicative of negative review:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>coeff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15030</th>\n",
       "      <td>ruined</td>\n",
       "      <td>-1.506146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15031</th>\n",
       "      <td>worse</td>\n",
       "      <td>-1.535246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15032</th>\n",
       "      <td>shit</td>\n",
       "      <td>-1.628900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15033</th>\n",
       "      <td>damn</td>\n",
       "      <td>-1.673210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15034</th>\n",
       "      <td>hate</td>\n",
       "      <td>-1.750861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15035</th>\n",
       "      <td>#sarcasm</td>\n",
       "      <td>-1.763777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15036</th>\n",
       "      <td>sad</td>\n",
       "      <td>-1.789984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15037</th>\n",
       "      <td>:(</td>\n",
       "      <td>-2.073988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15038</th>\n",
       "      <td>fuck</td>\n",
       "      <td>-2.082258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15039</th>\n",
       "      <td>worst</td>\n",
       "      <td>-2.105061</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           word     coeff\n",
       "15030    ruined -1.506146\n",
       "15031     worse -1.535246\n",
       "15032      shit -1.628900\n",
       "15033      damn -1.673210\n",
       "15034      hate -1.750861\n",
       "15035  #sarcasm -1.763777\n",
       "15036       sad -1.789984\n",
       "15037        :( -2.073988\n",
       "15038      fuck -2.082258\n",
       "15039     worst -2.105061"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# most negative words\n",
    "feature_coeffs.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_coeffs[\"abs_coeff\"] = feature_coeffs.coeff.abs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most predictive words, regardless of polarity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>coeff</th>\n",
       "      <th>abs_coeff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>best</td>\n",
       "      <td>2.236870</td>\n",
       "      <td>2.236870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>awesome</td>\n",
       "      <td>2.126289</td>\n",
       "      <td>2.126289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15039</th>\n",
       "      <td>worst</td>\n",
       "      <td>-2.105061</td>\n",
       "      <td>2.105061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15038</th>\n",
       "      <td>fuck</td>\n",
       "      <td>-2.082258</td>\n",
       "      <td>2.082258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15037</th>\n",
       "      <td>:(</td>\n",
       "      <td>-2.073988</td>\n",
       "      <td>2.073988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>:)</td>\n",
       "      <td>1.973052</td>\n",
       "      <td>1.973052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>amazing</td>\n",
       "      <td>1.842102</td>\n",
       "      <td>1.842102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fun</td>\n",
       "      <td>1.838183</td>\n",
       "      <td>1.838183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15036</th>\n",
       "      <td>sad</td>\n",
       "      <td>-1.789984</td>\n",
       "      <td>1.789984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15035</th>\n",
       "      <td>#sarcasm</td>\n",
       "      <td>-1.763777</td>\n",
       "      <td>1.763777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15034</th>\n",
       "      <td>hate</td>\n",
       "      <td>-1.750861</td>\n",
       "      <td>1.750861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>love</td>\n",
       "      <td>1.736795</td>\n",
       "      <td>1.736795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15033</th>\n",
       "      <td>damn</td>\n",
       "      <td>-1.673210</td>\n",
       "      <td>1.673210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15032</th>\n",
       "      <td>shit</td>\n",
       "      <td>-1.628900</td>\n",
       "      <td>1.628900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15031</th>\n",
       "      <td>worse</td>\n",
       "      <td>-1.535246</td>\n",
       "      <td>1.535246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15030</th>\n",
       "      <td>ruined</td>\n",
       "      <td>-1.506146</td>\n",
       "      <td>1.506146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15029</th>\n",
       "      <td>ass</td>\n",
       "      <td>-1.479902</td>\n",
       "      <td>1.479902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15028</th>\n",
       "      <td>bad</td>\n",
       "      <td>-1.456842</td>\n",
       "      <td>1.456842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15027</th>\n",
       "      <td>disappointed</td>\n",
       "      <td>-1.440478</td>\n",
       "      <td>1.440478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>forget</td>\n",
       "      <td>1.433278</td>\n",
       "      <td>1.433278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15026</th>\n",
       "      <td>ira</td>\n",
       "      <td>-1.428762</td>\n",
       "      <td>1.428762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15025</th>\n",
       "      <td>smh</td>\n",
       "      <td>-1.419773</td>\n",
       "      <td>1.419773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15024</th>\n",
       "      <td>lost</td>\n",
       "      <td>-1.405102</td>\n",
       "      <td>1.405102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15023</th>\n",
       "      <td>muslims</td>\n",
       "      <td>-1.387298</td>\n",
       "      <td>1.387298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15022</th>\n",
       "      <td>sucks</td>\n",
       "      <td>-1.366219</td>\n",
       "      <td>1.366219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15021</th>\n",
       "      <td>not</td>\n",
       "      <td>-1.343168</td>\n",
       "      <td>1.343168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15020</th>\n",
       "      <td>failed</td>\n",
       "      <td>-1.322501</td>\n",
       "      <td>1.322501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15019</th>\n",
       "      <td>yakub</td>\n",
       "      <td>-1.321549</td>\n",
       "      <td>1.321549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>excited</td>\n",
       "      <td>1.288761</td>\n",
       "      <td>1.288761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>sting</td>\n",
       "      <td>1.265486</td>\n",
       "      <td>1.265486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>proud</td>\n",
       "      <td>1.067714</td>\n",
       "      <td>1.067714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14995</th>\n",
       "      <td>instead</td>\n",
       "      <td>-1.066290</td>\n",
       "      <td>1.066290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>loved</td>\n",
       "      <td>1.063837</td>\n",
       "      <td>1.063837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>better</td>\n",
       "      <td>1.059014</td>\n",
       "      <td>1.059014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>thank</td>\n",
       "      <td>1.054869</td>\n",
       "      <td>1.054869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>lesnar</td>\n",
       "      <td>1.054841</td>\n",
       "      <td>1.054841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>beckham</td>\n",
       "      <td>1.053626</td>\n",
       "      <td>1.053626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14994</th>\n",
       "      <td>hard</td>\n",
       "      <td>-1.052672</td>\n",
       "      <td>1.052672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14993</th>\n",
       "      <td>crap</td>\n",
       "      <td>-1.051732</td>\n",
       "      <td>1.051732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14992</th>\n",
       "      <td>poor</td>\n",
       "      <td>-1.048424</td>\n",
       "      <td>1.048424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>thor</td>\n",
       "      <td>1.042675</td>\n",
       "      <td>1.042675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>ant-man</td>\n",
       "      <td>1.041877</td>\n",
       "      <td>1.041877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14991</th>\n",
       "      <td>gay</td>\n",
       "      <td>-1.034060</td>\n",
       "      <td>1.034060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14990</th>\n",
       "      <td>nooo</td>\n",
       "      <td>-1.028116</td>\n",
       "      <td>1.028116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14989</th>\n",
       "      <td>warning</td>\n",
       "      <td>-1.026093</td>\n",
       "      <td>1.026093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14988</th>\n",
       "      <td>planned</td>\n",
       "      <td>-1.024225</td>\n",
       "      <td>1.024225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14987</th>\n",
       "      <td>told</td>\n",
       "      <td>-1.023492</td>\n",
       "      <td>1.023492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14986</th>\n",
       "      <td>cameron</td>\n",
       "      <td>-1.016141</td>\n",
       "      <td>1.016141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14985</th>\n",
       "      <td>:/</td>\n",
       "      <td>-1.015069</td>\n",
       "      <td>1.015069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14984</th>\n",
       "      <td>away</td>\n",
       "      <td>-1.010865</td>\n",
       "      <td>1.010865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>greatest</td>\n",
       "      <td>0.994320</td>\n",
       "      <td>0.994320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14983</th>\n",
       "      <td>sorry</td>\n",
       "      <td>-0.992126</td>\n",
       "      <td>0.992126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14982</th>\n",
       "      <td>tory</td>\n",
       "      <td>-0.991799</td>\n",
       "      <td>0.991799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14981</th>\n",
       "      <td>dumb</td>\n",
       "      <td>-0.991432</td>\n",
       "      <td>0.991432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>80s</td>\n",
       "      <td>0.980261</td>\n",
       "      <td>0.980261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14980</th>\n",
       "      <td>suck</td>\n",
       "      <td>-0.978562</td>\n",
       "      <td>0.978562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14979</th>\n",
       "      <td>c'mon</td>\n",
       "      <td>-0.975269</td>\n",
       "      <td>0.975269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>brown</td>\n",
       "      <td>0.973063</td>\n",
       "      <td>0.973063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>win</td>\n",
       "      <td>0.971909</td>\n",
       "      <td>0.971909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14978</th>\n",
       "      <td>face</td>\n",
       "      <td>-0.965149</td>\n",
       "      <td>0.965149</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               word     coeff  abs_coeff\n",
       "0              best  2.236870   2.236870\n",
       "1           awesome  2.126289   2.126289\n",
       "15039         worst -2.105061   2.105061\n",
       "15038          fuck -2.082258   2.082258\n",
       "15037            :( -2.073988   2.073988\n",
       "2                :)  1.973052   1.973052\n",
       "3           amazing  1.842102   1.842102\n",
       "4               fun  1.838183   1.838183\n",
       "15036           sad -1.789984   1.789984\n",
       "15035      #sarcasm -1.763777   1.763777\n",
       "15034          hate -1.750861   1.750861\n",
       "5              love  1.736795   1.736795\n",
       "15033          damn -1.673210   1.673210\n",
       "15032          shit -1.628900   1.628900\n",
       "15031         worse -1.535246   1.535246\n",
       "15030        ruined -1.506146   1.506146\n",
       "15029           ass -1.479902   1.479902\n",
       "15028           bad -1.456842   1.456842\n",
       "15027  disappointed -1.440478   1.440478\n",
       "6            forget  1.433278   1.433278\n",
       "15026           ira -1.428762   1.428762\n",
       "15025           smh -1.419773   1.419773\n",
       "15024          lost -1.405102   1.405102\n",
       "15023       muslims -1.387298   1.387298\n",
       "15022         sucks -1.366219   1.366219\n",
       "15021           not -1.343168   1.343168\n",
       "15020        failed -1.322501   1.322501\n",
       "15019         yakub -1.321549   1.321549\n",
       "7           excited  1.288761   1.288761\n",
       "8             sting  1.265486   1.265486\n",
       "...             ...       ...        ...\n",
       "26            proud  1.067714   1.067714\n",
       "14995       instead -1.066290   1.066290\n",
       "27            loved  1.063837   1.063837\n",
       "28           better  1.059014   1.059014\n",
       "29            thank  1.054869   1.054869\n",
       "30           lesnar  1.054841   1.054841\n",
       "31          beckham  1.053626   1.053626\n",
       "14994          hard -1.052672   1.052672\n",
       "14993          crap -1.051732   1.051732\n",
       "14992          poor -1.048424   1.048424\n",
       "32             thor  1.042675   1.042675\n",
       "33          ant-man  1.041877   1.041877\n",
       "14991           gay -1.034060   1.034060\n",
       "14990          nooo -1.028116   1.028116\n",
       "14989       warning -1.026093   1.026093\n",
       "14988       planned -1.024225   1.024225\n",
       "14987          told -1.023492   1.023492\n",
       "14986       cameron -1.016141   1.016141\n",
       "14985            :/ -1.015069   1.015069\n",
       "14984          away -1.010865   1.010865\n",
       "34         greatest  0.994320   0.994320\n",
       "14983         sorry -0.992126   0.992126\n",
       "14982          tory -0.991799   0.991799\n",
       "14981          dumb -0.991432   0.991432\n",
       "35              80s  0.980261   0.980261\n",
       "14980          suck -0.978562   0.978562\n",
       "14979         c'mon -0.975269   0.975269\n",
       "36            brown  0.973063   0.973063\n",
       "37              win  0.971909   0.971909\n",
       "14978          face -0.965149   0.965149\n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_coeffs.sort_values(by = \"abs_coeff\",\n",
    "                           inplace = True,\n",
    "                           ascending = False)\n",
    "feature_coeffs.head(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Useless words (dont tell you anything about the polarity of the review):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>coeff</th>\n",
       "      <th>abs_coeff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7239</th>\n",
       "      <td>2307656948</td>\n",
       "      <td>1.255211e-06</td>\n",
       "      <td>1.255211e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7240</th>\n",
       "      <td>walls</td>\n",
       "      <td>1.255211e-06</td>\n",
       "      <td>1.255211e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7242</th>\n",
       "      <td>unions</td>\n",
       "      <td>1.255211e-06</td>\n",
       "      <td>1.255211e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7237</th>\n",
       "      <td>4857113</td>\n",
       "      <td>1.255211e-06</td>\n",
       "      <td>1.255211e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7252</th>\n",
       "      <td>https://t.co/s5bmecq1cl</td>\n",
       "      <td>2.425759e-09</td>\n",
       "      <td>2.425759e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7258</th>\n",
       "      <td>05575680</td>\n",
       "      <td>2.425759e-09</td>\n",
       "      <td>2.425759e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7257</th>\n",
       "      <td>takin</td>\n",
       "      <td>2.425759e-09</td>\n",
       "      <td>2.425759e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7256</th>\n",
       "      <td>6256062323</td>\n",
       "      <td>2.425759e-09</td>\n",
       "      <td>2.425759e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7255</th>\n",
       "      <td>pros</td>\n",
       "      <td>2.425759e-09</td>\n",
       "      <td>2.425759e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7254</th>\n",
       "      <td>jihad</td>\n",
       "      <td>2.425759e-09</td>\n",
       "      <td>2.425759e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7253</th>\n",
       "      <td>2409479223</td>\n",
       "      <td>2.425759e-09</td>\n",
       "      <td>2.425759e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7248</th>\n",
       "      <td>6992025</td>\n",
       "      <td>2.425759e-09</td>\n",
       "      <td>2.425759e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7251</th>\n",
       "      <td>https://t.co/mczysvgas8</td>\n",
       "      <td>2.425759e-09</td>\n",
       "      <td>2.425759e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7250</th>\n",
       "      <td>4322841</td>\n",
       "      <td>2.425759e-09</td>\n",
       "      <td>2.425759e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7247</th>\n",
       "      <td>sherbet</td>\n",
       "      <td>2.425759e-09</td>\n",
       "      <td>2.425759e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7246</th>\n",
       "      <td>couples</td>\n",
       "      <td>2.425759e-09</td>\n",
       "      <td>2.425759e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7245</th>\n",
       "      <td>6398295674</td>\n",
       "      <td>2.425759e-09</td>\n",
       "      <td>2.425759e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7244</th>\n",
       "      <td>2289663692</td>\n",
       "      <td>2.425759e-09</td>\n",
       "      <td>2.425759e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7243</th>\n",
       "      <td>enrole</td>\n",
       "      <td>2.425759e-09</td>\n",
       "      <td>2.425759e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7249</th>\n",
       "      <td>5366400</td>\n",
       "      <td>2.425759e-09</td>\n",
       "      <td>2.425759e-09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         word         coeff     abs_coeff\n",
       "7239               2307656948  1.255211e-06  1.255211e-06\n",
       "7240                    walls  1.255211e-06  1.255211e-06\n",
       "7242                   unions  1.255211e-06  1.255211e-06\n",
       "7237                  4857113  1.255211e-06  1.255211e-06\n",
       "7252  https://t.co/s5bmecq1cl  2.425759e-09  2.425759e-09\n",
       "7258                 05575680  2.425759e-09  2.425759e-09\n",
       "7257                    takin  2.425759e-09  2.425759e-09\n",
       "7256               6256062323  2.425759e-09  2.425759e-09\n",
       "7255                     pros  2.425759e-09  2.425759e-09\n",
       "7254                    jihad  2.425759e-09  2.425759e-09\n",
       "7253               2409479223  2.425759e-09  2.425759e-09\n",
       "7248                  6992025  2.425759e-09  2.425759e-09\n",
       "7251  https://t.co/mczysvgas8  2.425759e-09  2.425759e-09\n",
       "7250                  4322841  2.425759e-09  2.425759e-09\n",
       "7247                  sherbet  2.425759e-09  2.425759e-09\n",
       "7246                  couples  2.425759e-09  2.425759e-09\n",
       "7245               6398295674  2.425759e-09  2.425759e-09\n",
       "7244               2289663692  2.425759e-09  2.425759e-09\n",
       "7243                   enrole  2.425759e-09  2.425759e-09\n",
       "7249                  5366400  2.425759e-09  2.425759e-09"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_coeffs.tail(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipelines To Make CV/Transformations Easier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_pipeline = Pipeline([(\"countVect\", # string that explains what it does\n",
    "                            CountVectorizer(tokenizer = tokenizer_for_tweets.tokenize)), # step 1\n",
    "                           (\"lr\",LogisticRegression())]) # step 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('countVect', CountVectorizer(analyzer=u'word', binary=False, decode_error=u'strict',\n",
       "        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "     ...ty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False))])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# passing the raw data through the defined pipeline\n",
    "first_pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.81\n"
     ]
    }
   ],
   "source": [
    "# accuracy based on actual testing data and prediction \n",
    "## \".predict\" indicates it's an estimator\n",
    "print(metrics.accuracy_score(y_test, first_pipeline.predict(X_test)).round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "strat_cv = StratifiedKFold(n_splits=10) # balance splits by frequency of each target class\n",
    "# cv = 10 #10-fold cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:    6.3s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.81"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# average accuracy over 10 strafied cross-validations \n",
    "np.mean(cross_val_score(first_pipeline,\n",
    "                semeval_data.tweet,\n",
    "                semeval_data.target,\n",
    "                scoring = \"accuracy\",\n",
    "                cv = strat_cv,\n",
    "                n_jobs = -1, # parallel processing\n",
    "                verbose = 1)).round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('countVect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        ...ty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False))])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fitting data in the entire dataset\n",
    "first_pipeline.fit(semeval_data.tweet, semeval_data.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['!',\n",
       " '\"',\n",
       " '#',\n",
       " '#039',\n",
       " '#12',\n",
       " '#1381619',\n",
       " '#14thofmillions',\n",
       " '#15philippines',\n",
       " '#15thshot',\n",
       " '#1a',\n",
       " '#21053',\n",
       " '#39',\n",
       " '#3dt',\n",
       " '#48',\n",
       " '#4th',\n",
       " '#5monthswithoutzayn',\n",
       " '#6hobbs',\n",
       " '#99centicedcoffee',\n",
       " '#aactas',\n",
       " '#abouttime',\n",
       " '#ad',\n",
       " '#addicted',\n",
       " '#ade',\n",
       " '#adultlife',\n",
       " '#afailedtextfromtoday',\n",
       " '#afc',\n",
       " '#afcvcfc',\n",
       " '#afcvhcfc',\n",
       " '#afhv',\n",
       " '#africa',\n",
       " '#africapkenya',\n",
       " '#aggies',\n",
       " '#ahhh',\n",
       " '#alessandra',\n",
       " '#alldaybreakfast',\n",
       " '#allregion',\n",
       " '#allstar',\n",
       " '#amateur',\n",
       " '#amateurchampion',\n",
       " '#amazing',\n",
       " '#amazingamateur',\n",
       " '#amazon',\n",
       " '#amazonprimeday',\n",
       " '#amen',\n",
       " '#amreading',\n",
       " '#android',\n",
       " '#andy_x9',\n",
       " '#andymurray',\n",
       " '#angels',\n",
       " '#annahazare',\n",
       " '#anniesboobs',\n",
       " '#annoyed',\n",
       " '#anthonychat',\n",
       " '#apartheid',\n",
       " '#appleevent',\n",
       " '#apps',\n",
       " '#aprildies',\n",
       " '#aprillives',\n",
       " '#areyoufreakingkiddingme',\n",
       " '#arrow',\n",
       " '#arsenal',\n",
       " '#asatru',\n",
       " '#ascend',\n",
       " '#asematy',\n",
       " '#ashamed',\n",
       " '#askbobby',\n",
       " '#askcairney',\n",
       " '#asthmaticproblems',\n",
       " '#astros',\n",
       " '#asu',\n",
       " '#ateam',\n",
       " '#atlanticcity',\n",
       " '#auktop20',\n",
       " '#aurora',\n",
       " '#auroraborealis',\n",
       " '#auspol',\n",
       " '#australia',\n",
       " '#australiawantsdemi',\n",
       " '#autocar',\n",
       " '#avb',\n",
       " '#avbout',\n",
       " '#avfc',\n",
       " '#avoidee',\n",
       " '#awakenthenorth',\n",
       " '#awaydays',\n",
       " '#awesome',\n",
       " '#awkward',\n",
       " '#awkwardacting',\n",
       " '#ayto',\n",
       " '#bachelorau',\n",
       " '#badass',\n",
       " '#badboy4life',\n",
       " '#baddecisions',\n",
       " '#badeditor',\n",
       " '#badlands',\n",
       " '#badluck',\n",
       " '#badmovie',\n",
       " '#bajrangibhaijaan',\n",
       " '#balotelli',\n",
       " '#banglyphosate',\n",
       " '#bangmo',\n",
       " '#banpesticides',\n",
       " '#banroundup',\n",
       " '#bars',\n",
       " '#batman',\n",
       " '#bb17',\n",
       " '#bb18',\n",
       " '#bbcdp',\n",
       " '#bbcnews',\n",
       " '#bbcqt',\n",
       " '#bbcsp',\n",
       " '#bbn',\n",
       " '#bbwhereareyou',\n",
       " '#bds',\n",
       " '#beachbum',\n",
       " '#beautiful',\n",
       " '#beautifulwords',\n",
       " '#beckham',\n",
       " '#behappy',\n",
       " '#believe',\n",
       " '#believeinbritain',\n",
       " '#bellissimo',\n",
       " '#benghazi',\n",
       " '#beoriginal',\n",
       " '#bernie2016',\n",
       " '#bestfriend',\n",
       " '#bestinlombok',\n",
       " '#bestlunche',\n",
       " '#bestmixtapesever',\n",
       " '#besttimeoftheday',\n",
       " '#bet',\n",
       " '#bethiphopawards',\n",
       " '#betonaldo',\n",
       " '#bettreysongz',\n",
       " '#bgsummerslam',\n",
       " '#biasprick',\n",
       " '#bieber',\n",
       " '#bigbrother',\n",
       " '#bigbrother17',\n",
       " '#bigdata',\n",
       " '#bigot',\n",
       " '#bigrevivaltour',\n",
       " '#billandted',\n",
       " '#billboard',\n",
       " '#bills',\n",
       " '#billsmafia',\n",
       " '#birthdayboy',\n",
       " '#bitteroldmantweet',\n",
       " '#bizikent',\n",
       " '#bizitalk',\n",
       " '#bjpblackmoneydhokha',\n",
       " '#black',\n",
       " '#blackbirdgang',\n",
       " '#blackday',\n",
       " '#blackfriday',\n",
       " '#blackhistorymoment',\n",
       " '#blacklivesmatter',\n",
       " '#blackmail',\n",
       " '#blag',\n",
       " '#bloggerperks',\n",
       " '#bloomberg',\n",
       " '#bluejays',\n",
       " '#bnppo12',\n",
       " '#bo66y',\n",
       " '#boilingmad',\n",
       " '#bokoharam',\n",
       " '#boldindenver',\n",
       " \"#bondgeek's\",\n",
       " '#boogersugarftw',\n",
       " '#bored',\n",
       " '#boring',\n",
       " '#borutothemovie',\n",
       " '#bossypants',\n",
       " '#bostonfest',\n",
       " '#botox',\n",
       " '#boxing101',\n",
       " '#boxoffice',\n",
       " '#bpl',\n",
       " '#bpos',\n",
       " '#bradyrules',\n",
       " '#bravo',\n",
       " '#brawl',\n",
       " '#breakingpointe',\n",
       " '#brewers',\n",
       " '#bringbacksungwoo',\n",
       " '#bringonthesun',\n",
       " '#britishopen',\n",
       " '#bro',\n",
       " '#brocklesnar',\n",
       " '#brockvstaker',\n",
       " '#brother',\n",
       " '#bruh',\n",
       " '#bruinfam',\n",
       " '#bsff',\n",
       " '#btronpcas',\n",
       " '#burdwanblast',\n",
       " '#burnaby',\n",
       " '#burningman',\n",
       " '#butilovethemsomuch',\n",
       " '#bwfc',\n",
       " '#bye',\n",
       " '#byewalker',\n",
       " '#cadillac',\n",
       " '#cafc',\n",
       " '#caitlynjenner',\n",
       " '#cakecrawl',\n",
       " '#calibraskaep',\n",
       " '#callmecrazy',\n",
       " '#canadiancellphoneplanssuck',\n",
       " '#cancelmonday',\n",
       " '#cannabis',\n",
       " '#canttakethisanymore',\n",
       " '#cantwait',\n",
       " '#cantyouworkoutmore',\n",
       " '#capeclothconvo',\n",
       " '#captainamerica',\n",
       " '#cardassians',\n",
       " '#cardealership',\n",
       " '#cardiff',\n",
       " '#cardinals',\n",
       " '#carkit',\n",
       " '#carly2016',\n",
       " '#carol',\n",
       " '#castlecrashers',\n",
       " '#castsucks',\n",
       " '#catfished',\n",
       " '#catholicchurch',\n",
       " '#catmom',\n",
       " '#cbb',\n",
       " '#ccot',\n",
       " '#ccs',\n",
       " '#cdnpoli',\n",
       " '#celebrity',\n",
       " '#cena',\n",
       " '#cenation',\n",
       " '#cenavsrollins',\n",
       " '#cfc',\n",
       " '#cfr12',\n",
       " '#challengeaccepted',\n",
       " '#cheater',\n",
       " '#cheaters',\n",
       " '#cheesymusic',\n",
       " '#chewie',\n",
       " '#chicagomagicpsg',\n",
       " '#chiefs',\n",
       " '#childrenslibrarian',\n",
       " '#china',\n",
       " '#choke',\n",
       " '#christmas',\n",
       " '#chsocm',\n",
       " '#ciclavia',\n",
       " '#cimbclassic',\n",
       " '#cincytennis',\n",
       " '#cinderella',\n",
       " '#cinema',\n",
       " '#cineworld',\n",
       " '#ckrcomp',\n",
       " '#classic',\n",
       " '#climate',\n",
       " '#clown',\n",
       " '#clueless',\n",
       " '#cmon',\n",
       " '#cocktails',\n",
       " '#coffee',\n",
       " '#comeback',\n",
       " '#comeonengland',\n",
       " '#comeonyoublues',\n",
       " '#cometogether',\n",
       " '#comm552',\n",
       " '#committed',\n",
       " '#completerichness',\n",
       " '#complimentary',\n",
       " '#compton',\n",
       " '#confidence',\n",
       " '#confirmed',\n",
       " '#confused',\n",
       " '#cool',\n",
       " '#coolestbeatle',\n",
       " '#corbyn',\n",
       " '#corbyn4leader',\n",
       " '#cornhusker',\n",
       " '#couldnthelpmyself',\n",
       " '#countdowntokickoff',\n",
       " '#covingtonla',\n",
       " '#cowboys',\n",
       " '#coybig',\n",
       " '#coyg',\n",
       " '#coys',\n",
       " '#cpfc',\n",
       " '#cre',\n",
       " '#cricket',\n",
       " '#crime',\n",
       " '#cripples',\n",
       " '#crushsohard',\n",
       " '#crybaby',\n",
       " '#cryingforever',\n",
       " '#crystal',\n",
       " '#csrracing',\n",
       " '#cubs',\n",
       " '#cubstalk',\n",
       " '#curtises',\n",
       " '#cyclonenation',\n",
       " '#d810',\n",
       " '#dabears',\n",
       " '#daesh',\n",
       " '#dafuckouttahere',\n",
       " '#dailynba',\n",
       " '#darkpassenger',\n",
       " '#dastal',\n",
       " '#dawnsearlylight',\n",
       " '#days',\n",
       " '#days50',\n",
       " '#dbacks',\n",
       " '#debate',\n",
       " '#defundpp',\n",
       " '#delightdelaysweeps',\n",
       " '#deltaview',\n",
       " '#deluded',\n",
       " '#demdebate',\n",
       " '#denali',\n",
       " '#depressing',\n",
       " '#describeyourphobia',\n",
       " '#destiny',\n",
       " '#dexter',\n",
       " '#dfs',\n",
       " '#directionersfuneral',\n",
       " '#discount',\n",
       " '#diversity2',\n",
       " '#dmv',\n",
       " '#dobetterbywomen',\n",
       " '#doctorwho',\n",
       " '#documentthiswwe',\n",
       " '#dohertyindisguise',\n",
       " '#doingwork',\n",
       " '#dolocal',\n",
       " '#dominos',\n",
       " '#donate',\n",
       " '#done',\n",
       " '#dontbombsyria',\n",
       " '#dontbuythesun',\n",
       " '#donthateme',\n",
       " '#dontmissit',\n",
       " '#dope',\n",
       " '#dorset',\n",
       " '#dragmedownmusicvideo',\n",
       " '#dragmedownmusicvideoisout',\n",
       " '#dragmedownvevorecord',\n",
       " '#dragoncon',\n",
       " '#dreamy',\n",
       " '#drillers',\n",
       " '#drinkalittledrink',\n",
       " '#drinklocal',\n",
       " '#dronegate',\n",
       " '#drop',\n",
       " '#drought',\n",
       " '#drugs',\n",
       " '#drunk',\n",
       " '#dubw',\n",
       " '#ducks',\n",
       " '#duh',\n",
       " '#duh-un',\n",
       " '#duli11',\n",
       " '#dumasswalker',\n",
       " '#dumbass',\n",
       " '#dundascactus40',\n",
       " '#dustinjohnson',\n",
       " '#dw_english',\n",
       " '#dying',\n",
       " '#eaglenation',\n",
       " '#earnabike',\n",
       " '#earthquake',\n",
       " '#ebola',\n",
       " '#econokit',\n",
       " '#edchat',\n",
       " '#edfringe',\n",
       " '#edl',\n",
       " '#edo',\n",
       " '#edsheeran',\n",
       " '#education',\n",
       " '#egypt',\n",
       " '#eidldn',\n",
       " '#elxn42',\n",
       " '#emabiggestfans1d',\n",
       " '#endthelockout',\n",
       " '#energyefficiency',\n",
       " '#eng',\n",
       " '#england',\n",
       " '#engvswi',\n",
       " '#enjoy',\n",
       " '#entertaining',\n",
       " '#entrepreneurship',\n",
       " '#eotpro',\n",
       " '#epl',\n",
       " '#escalade',\n",
       " '#espnblockedatourhouse',\n",
       " '#espnnow',\n",
       " '#espnsucks',\n",
       " '#evalesco',\n",
       " '#everton',\n",
       " '#excited',\n",
       " '#excusememr',\n",
       " '#expo15nhs',\n",
       " '#eyeswithpride',\n",
       " '#f1',\n",
       " '#f12014game',\n",
       " '#facepalm',\n",
       " '#facetoface',\n",
       " '#fact',\n",
       " '#facts',\n",
       " '#faff',\n",
       " '#fail',\n",
       " '#fakeintolerance',\n",
       " '#fakta',\n",
       " '#fallacy',\n",
       " '#fallharvest',\n",
       " '#family',\n",
       " '#famished',\n",
       " '#fantasybasketball',\n",
       " '#fastfooddaysofxmas',\n",
       " '#fawsl',\n",
       " '#fb',\n",
       " '#fcb',\n",
       " '#fcblive',\n",
       " '#fcfilmmedia',\n",
       " '#feelgodlove',\n",
       " '#feelthebern',\n",
       " '#feminism',\n",
       " '#fib',\n",
       " '#fightingtalk',\n",
       " '#figureitout',\n",
       " '#film',\n",
       " '#filmmaking',\n",
       " '#findarealsportsstory',\n",
       " '#finder',\n",
       " '#fingerfood',\n",
       " '#fingerscrossed',\n",
       " '#fire',\n",
       " '#firstft',\n",
       " '#flying',\n",
       " '#flyinghigh',\n",
       " '#fml',\n",
       " '#fmlll',\n",
       " '#foodjustice',\n",
       " '#foofighters',\n",
       " '#football',\n",
       " '#for',\n",
       " '#forceawakens',\n",
       " '#forcefriday',\n",
       " '#forwhatthatsworth',\n",
       " '#forzamilan',\n",
       " '#foxnews',\n",
       " '#frankgifford',\n",
       " '#freakinweekend',\n",
       " '#freedogs',\n",
       " '#freeghoncheh',\n",
       " '#freepizza',\n",
       " '#freshkicks',\n",
       " '#friday',\n",
       " '#fridayvibes',\n",
       " '#friends',\n",
       " '#friendshipgoals',\n",
       " '#fsu',\n",
       " '#ftm',\n",
       " '#fuck',\n",
       " '#fuckyouviagogo',\n",
       " '#fumatubloodline',\n",
       " '#funfacts',\n",
       " '#funtimes',\n",
       " '#gaa',\n",
       " '#gagaupdate',\n",
       " '#gaming',\n",
       " '#gamlingay',\n",
       " '#gangshit',\n",
       " '#gawker',\n",
       " '#gaza',\n",
       " '#gchq',\n",
       " '#gdp',\n",
       " '#ge16',\n",
       " '#ge2020',\n",
       " '#gearsofwarultimateedition',\n",
       " '#geographyteacher',\n",
       " '#germany',\n",
       " '#getairtel4g',\n",
       " '#getfucked',\n",
       " '#getherdunne',\n",
       " '#getmoving',\n",
       " '#getshawntoportugal',\n",
       " '#gfyh',\n",
       " '#gh',\n",
       " '#giantsfootball',\n",
       " '#globalcitizen',\n",
       " '#goalmachine',\n",
       " '#gobears',\n",
       " '#godblessnigeria',\n",
       " '#godeacs',\n",
       " '#godhelpme',\n",
       " '#godpunishu',\n",
       " '#goduke',\n",
       " '#gogaga',\n",
       " '#gogetem',\n",
       " '#goggleeyedhomonculus',\n",
       " '#goldenopportunity',\n",
       " '#golouisville',\n",
       " '#golow',\n",
       " '#gomariners',\n",
       " '#goodday',\n",
       " '#gooddays',\n",
       " '#goodmood',\n",
       " '#goodnightlolla',\n",
       " '#goodtimes',\n",
       " '#google',\n",
       " '#gop',\n",
       " '#gop2012',\n",
       " '#gopclowncar',\n",
       " '#gopdebate',\n",
       " '#gopfail',\n",
       " '#gotmypictureforinstagramready',\n",
       " '#gqmenoftheyear2015',\n",
       " '#gqmoty2015',\n",
       " '#grammy',\n",
       " '#grammynoms',\n",
       " '#grammys',\n",
       " '#gratefuldead',\n",
       " '#greatness',\n",
       " '#greatwork',\n",
       " '#greece',\n",
       " '#griffithvotes',\n",
       " '#grindin',\n",
       " '#groworganic',\n",
       " '#gsaw',\n",
       " '#gsow',\n",
       " '#guilty',\n",
       " '#guiltypleasure',\n",
       " '#gunsense',\n",
       " '#gurdaspur',\n",
       " '#gurdaspurattack',\n",
       " '#gwu',\n",
       " '#halifax',\n",
       " '#halloween',\n",
       " '#hamont',\n",
       " '#han',\n",
       " '#hangyakub',\n",
       " '#hannibal',\n",
       " '#happy',\n",
       " '#happy4thanniversarykathniel',\n",
       " '#happybirthdayprincegeorge',\n",
       " '#happydays',\n",
       " '#happyeid',\n",
       " '#happymonday',\n",
       " '#happyprimeday',\n",
       " '#hate',\n",
       " '#hatembomb',\n",
       " '#havefun',\n",
       " '#headies',\n",
       " '#headiesall',\n",
       " '#hearthorses',\n",
       " '#heartnews',\n",
       " '#heartofmidlothian',\n",
       " '#hearts',\n",
       " '#heaven',\n",
       " '#hehasfourbrot',\n",
       " '#herestoanother',\n",
       " '#hermes',\n",
       " '#heymissparker',\n",
       " '#hhn25',\n",
       " '#hibeees',\n",
       " '#hibs',\n",
       " '#higherlearning',\n",
       " '#highhopes',\n",
       " '#hilarious',\n",
       " '#hillary',\n",
       " '#hillary2016',\n",
       " '#hillaryclinton',\n",
       " '#hipsters',\n",
       " '#hivprevention',\n",
       " '#hlwd',\n",
       " '#hmfc',\n",
       " '#hockey',\n",
       " '#hockeyisback',\n",
       " '#hollywood',\n",
       " '#hololens',\n",
       " '#homecoming',\n",
       " '#homeworkcanwait',\n",
       " '#homs',\n",
       " '#honeymoon',\n",
       " '#hookem',\n",
       " '#hopeful',\n",
       " '#horn',\n",
       " '#hou',\n",
       " '#hsbcfraud',\n",
       " '#hsonair',\n",
       " '#hungary',\n",
       " '#hurricanesandy',\n",
       " '#huskers',\n",
       " '#hydrogen',\n",
       " '#hypocrites',\n",
       " '#ibmcsc',\n",
       " '#ibmmiddleware',\n",
       " '#ibmsports',\n",
       " '#ibmstu15',\n",
       " '#ibmtraining',\n",
       " '#icecreamday',\n",
       " '#ichigono',\n",
       " '#icymi',\n",
       " '#idiocracy',\n",
       " '#idiots',\n",
       " '#idontwantit',\n",
       " '#igotitfree',\n",
       " '#ihateschool',\n",
       " '#ihavethepower',\n",
       " '#ihope',\n",
       " '#ijustnerdedsohardmyglassesbroke',\n",
       " '#iknowimnottheonlyone',\n",
       " '#iloveamerica',\n",
       " '#iloveyourg3',\n",
       " '#imagine',\n",
       " '#imatmybreakingpoint',\n",
       " '#imf',\n",
       " '#immigrantlivesmatter',\n",
       " '#independent',\n",
       " '#india',\n",
       " '#indians',\n",
       " '#indiedev',\n",
       " '#indvwi',\n",
       " '#inked',\n",
       " '#inmiddeschool',\n",
       " '#inners',\n",
       " '#inpodwetrust',\n",
       " '#interestingfactoftheday',\n",
       " '#interestinggilfriend',\n",
       " '#ipad',\n",
       " '#iparty',\n",
       " '#ipho',\n",
       " '#iran',\n",
       " '#irandeal',\n",
       " '#iranresistance',\n",
       " '#iraq2003',\n",
       " '#irish',\n",
       " '#ironmaiden',\n",
       " '#ishouldjustwatchthat',\n",
       " '#isis',\n",
       " '#isitok',\n",
       " '#it',\n",
       " '#itfc',\n",
       " '#itoa',\n",
       " '#itsbeentolong',\n",
       " '#itsm',\n",
       " '#itsmissusa',\n",
       " '#itssoearly',\n",
       " '#itunes',\n",
       " '#iwantaamiparkback',\n",
       " '#iwillneverunderstandwhy',\n",
       " '#jacku',\n",
       " '#jail',\n",
       " '#jamaica',\n",
       " '#jan1',\n",
       " '#jasonaldeantoday',\n",
       " '#javedhashmi',\n",
       " '#jays',\n",
       " '#jeremy4leader',\n",
       " '#jeremycorbyn',\n",
       " '#jerseyshore',\n",
       " '#jerzday',\n",
       " '#jets',\n",
       " '#jetsnation',\n",
       " '#jezwecan',\n",
       " '#jhub',\n",
       " '#john_cena',\n",
       " '#johnkerry',\n",
       " '#johnny',\n",
       " '#joker',\n",
       " '#jt',\n",
       " '#juiceplus',\n",
       " '#juryhouse',\n",
       " '#justasec',\n",
       " '#justsayin',\n",
       " '#justsaying',\n",
       " '#juvederm',\n",
       " '#k9charger',\n",
       " '#k9general',\n",
       " '#keepingmyshizztogether',\n",
       " '#kenwrightout',\n",
       " '#kerryforthedouble',\n",
       " '#kerryvdublin',\n",
       " '#khader',\n",
       " '#khaderexists',\n",
       " '#kill',\n",
       " '#killing',\n",
       " '#killmenow',\n",
       " '#kimdavies',\n",
       " '#kimdavis',\n",
       " '#kissmyassgreendaythebest',\n",
       " '#klgg',\n",
       " '#knicksnation',\n",
       " '#koran',\n",
       " '#kreygasm',\n",
       " '#kstate',\n",
       " '#kubball',\n",
       " '#laborday',\n",
       " '#labourdoorstep',\n",
       " '#lackofexperienceshows',\n",
       " '#laliga',\n",
       " '#lamar',\n",
       " '#lambily',\n",
       " '#lastfreemovieweekend',\n",
       " '#latechsuckd',\n",
       " '#latenightthoughts',\n",
       " '#lawyerswithpride',\n",
       " '#lazio',\n",
       " '#lchbuzz',\n",
       " '#ld',\n",
       " '#lean',\n",
       " '#leapinno',\n",
       " '#lease',\n",
       " '#leaside',\n",
       " '#leaveityeah',\n",
       " '#legend',\n",
       " '#legends',\n",
       " '#legohouse',\n",
       " '#lehgo',\n",
       " '#letmeseeyagirl',\n",
       " '#letsgetoursecondcarsout',\n",
       " '#letsgomets',\n",
       " '#lfc',\n",
       " '#lgbtq',\n",
       " '#lgm',\n",
       " '#lhsc',\n",
       " '#liannerocks',\n",
       " '#libdemfightback',\n",
       " '#lifeisgood',\n",
       " '#ligabbva',\n",
       " '#lilmix',\n",
       " '#linearlagebra',\n",
       " '#lions',\n",
       " '#listenup',\n",
       " '#listia',\n",
       " '#littlethings',\n",
       " '#littlewomenla',\n",
       " '#livefromthebanksoftheriverstyx',\n",
       " '#lml',\n",
       " '#loanwatch',\n",
       " '#local24',\n",
       " '#lol',\n",
       " '#lolla',\n",
       " '#lollapalooza',\n",
       " '#londonriots',\n",
       " '#lonewolf',\n",
       " '#loooser',\n",
       " '#looserpool',\n",
       " '#loudandproud',\n",
       " '#love',\n",
       " '#loveabitofbiff',\n",
       " '#lovemyjob',\n",
       " '#lowes48',\n",
       " '#lpc',\n",
       " '#lufc',\n",
       " '#lunch',\n",
       " '#lynndebate',\n",
       " '#m6',\n",
       " '#macjc',\n",
       " '#mad',\n",
       " '#madison',\n",
       " '#magazine',\n",
       " '#makeamericagreatagain',\n",
       " '#makeamove',\n",
       " '#makeitcount',\n",
       " '#makeitpossible',\n",
       " '#makeworkpay',\n",
       " '#maltese',\n",
       " '#mam',\n",
       " '#manchester',\n",
       " '#manchesterpride2015',\n",
       " '#mapleleafs',\n",
       " '#mardigras',\n",
       " '#mariners',\n",
       " '#marketing',\n",
       " '#marr',\n",
       " '#massive',\n",
       " '#math',\n",
       " '#mathewstaver',\n",
       " '#mathhomework',\n",
       " '#mattbomer',\n",
       " '#may',\n",
       " '#maybenextyear',\n",
       " '#maytheforcefacts',\n",
       " '#mccann',\n",
       " '#mcfashion',\n",
       " '#mcm',\n",
       " '#mdmadness',\n",
       " '#meangirls',\n",
       " '#meccalive',\n",
       " '#media',\n",
       " '#meekbelike',\n",
       " '#meetthepress',\n",
       " '#memories',\n",
       " '#mercedes',\n",
       " '#mercury2011',\n",
       " '#merdealamort',\n",
       " '#merrychristmas',\n",
       " '#mets',\n",
       " '#mexvusa',\n",
       " '#microsoft',\n",
       " '#middleschooljams',\n",
       " '#milan',\n",
       " '#mile',\n",
       " '#millennials',\n",
       " '#millwall',\n",
       " '#miracle',\n",
       " '#missherlots',\n",
       " '#mixtapecominsoon',\n",
       " '#mizzouspirit',\n",
       " '#mlb',\n",
       " '#mlbfantasy',\n",
       " '#mnf',\n",
       " '#mnwild',\n",
       " '#mockingjay',\n",
       " '#moderncurses',\n",
       " '#mohandas',\n",
       " '#mohrstories',\n",
       " '#monday',\n",
       " '#mondaydaze',\n",
       " '#mondaymorning',\n",
       " '#monsanto',\n",
       " '#mood',\n",
       " '#moonwalkdebut',\n",
       " '#morningjoe',\n",
       " '#motivation',\n",
       " '#motog',\n",
       " '#mountaineernation',\n",
       " '#movie',\n",
       " '#movieonthelawn',\n",
       " '#mtvhottest',\n",
       " '#mtvstars',\n",
       " '#mtvstarsof2015',\n",
       " '#multiplyclarkpub',\n",
       " '#murray',\n",
       " '#music',\n",
       " '#muslimsarenotdragqueens',\n",
       " '#mustpushthrough',\n",
       " '#mvfc',\n",
       " '#my',\n",
       " '#mybaby',\n",
       " '#myfavorite',\n",
       " '#mylove',\n",
       " '#myyouthin5words',\n",
       " '#nacamam',\n",
       " '#narendra',\n",
       " '#naruto',\n",
       " '#nascar',\n",
       " '#nationalherald',\n",
       " '#nationalhotdogday',\n",
       " '#nationaltequiladay',\n",
       " '#nba',\n",
       " '#ncaa',\n",
       " '#nccdc7',\n",
       " '#nebrasketball',\n",
       " '#needmoney',\n",
       " '#needstostop',\n",
       " '#needstructure',\n",
       " '#neither',\n",
       " '#net',\n",
       " '#net-zero',\n",
       " '#networkwithfmprofessionals',\n",
       " '#nevergetsold',\n",
       " '#new',\n",
       " '#newnickname',\n",
       " '#news',\n",
       " '#newser',\n",
       " '#newstraders',\n",
       " '#newyork',\n",
       " '#nfl',\n",
       " '#ng',\n",
       " '#ngu',\n",
       " '#nhl',\n",
       " '#nhllockout',\n",
       " '#nibiru',\n",
       " '#nightofchampions',\n",
       " '#nikeplus',\n",
       " '#nikon',\n",
       " '#ninasimone',\n",
       " '#nintendo',\n",
       " '#nintendo3ds',\n",
       " '#nlcs',\n",
       " '#nobusinessrunningforpresident',\n",
       " '#nocolbert',\n",
       " '#nocup',\n",
       " '#nola',\n",
       " '#nolie',\n",
       " '#nolife',\n",
       " '#nomoreexcuses',\n",
       " '#noonecares',\n",
       " '#norestforthewicked',\n",
       " '#noritzpro',\n",
       " '#norwich',\n",
       " '#not',\n",
       " '#notkardashians',\n",
       " '#notlookingforwardtodennys',\n",
       " '#notokay',\n",
       " '#notoptimal',\n",
       " '#notreally',\n",
       " '#notredame',\n",
       " '#notthatguy',\n",
       " '#nottheband',\n",
       " '#nowplaying',\n",
       " '#nowwatching',\n",
       " '#np',\n",
       " '#nud',\n",
       " '#nufc',\n",
       " '#nuffsaid',\n",
       " '#nw',\n",
       " '#nwom',\n",
       " '#nyc',\n",
       " '#nycdoe',\n",
       " '#nycproblems',\n",
       " '#nyfw',\n",
       " '#nygiants',\n",
       " '#nyj',\n",
       " '#nymets',\n",
       " '#nymvskc',\n",
       " '#nyr',\n",
       " '#nyulclassic',\n",
       " '#obama',\n",
       " '#obamafailed',\n",
       " '#occupyauc',\n",
       " '#oct3rd',\n",
       " '#october',\n",
       " '#officelife',\n",
       " '#officetalk',\n",
       " '#ogoplay',\n",
       " '#ohhellno',\n",
       " '#okay',\n",
       " '#oldmaneady',\n",
       " '#one',\n",
       " '#oneday',\n",
       " '#oneparkcity',\n",
       " '#onevoicethetour',\n",
       " '#onlyadream',\n",
       " '#onthisday1987',\n",
       " '#oomf',\n",
       " '#ooops',\n",
       " '#open2015',\n",
       " '#operationptl',\n",
       " '#ops',\n",
       " '#orioles',\n",
       " '#oscar',\n",
       " '#oshiomhole',\n",
       " '#otaku',\n",
       " '#otra',\n",
       " '#ouch',\n",
       " '#overrated',\n",
       " '#overturned',\n",
       " '#oww',\n",
       " '#p2',\n",
       " '#pacers',\n",
       " '#pakistani',\n",
       " '#pakvaus',\n",
       " '#panorama',\n",
       " '#papertownsmovie',\n",
       " '#parenting',\n",
       " '#paris',\n",
       " '#parisattacks',\n",
       " '#pascon',\n",
       " '#pathetic',\n",
       " '#patriots',\n",
       " '#paulmccartney',\n",
       " '#pawsox',\n",
       " '#peds',\n",
       " '#pennycan',\n",
       " '#pens',\n",
       " '#people',\n",
       " '#periscope',\n",
       " '#pgachampionship',\n",
       " '#pgatour',\n",
       " '#phillies',\n",
       " '#philly',\n",
       " '#phone',\n",
       " '#phrasethatpays',\n",
       " '#phxcc',\n",
       " '#pilove',\n",
       " '#pinchme',\n",
       " '#pizzaday',\n",
       " '#pizzasweats',\n",
       " '#pjnet',\n",
       " '#planetx',\n",
       " '#playing',\n",
       " '#playoffrace',\n",
       " '#plfans',\n",
       " '#plunginoracleday',\n",
       " '#pmqs',\n",
       " '#pointtexas',\n",
       " '#police',\n",
       " '#poorcustomerservice',\n",
       " '#poppinbottles',\n",
       " '#portsmouth',\n",
       " '#positive',\n",
       " '#poundinggrape',\n",
       " '#pplhdtv',\n",
       " '#prayforang',\n",
       " '#prayforparis',\n",
       " '#predictiveanalytics',\n",
       " ...]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to access the innards\n",
    "first_pipeline.steps[0][1].get_feature_names() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>coeff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>love</td>\n",
       "      <td>2.222775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>best</td>\n",
       "      <td>2.196226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>awesome</td>\n",
       "      <td>2.115709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>:)</td>\n",
       "      <td>2.049287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>amazing</td>\n",
       "      <td>1.993686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>fun</td>\n",
       "      <td>1.703092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>excited</td>\n",
       "      <td>1.623278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>happy</td>\n",
       "      <td>1.547635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>nice</td>\n",
       "      <td>1.499429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>beautiful</td>\n",
       "      <td>1.487371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>thank</td>\n",
       "      <td>1.486662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>good</td>\n",
       "      <td>1.408511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>great</td>\n",
       "      <td>1.360709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>perfect</td>\n",
       "      <td>1.324802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>cool</td>\n",
       "      <td>1.286487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>interesting</td>\n",
       "      <td>1.281344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>november</td>\n",
       "      <td>1.260746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>won</td>\n",
       "      <td>1.247853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>december</td>\n",
       "      <td>1.234652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>brilliant</td>\n",
       "      <td>1.213485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>holy</td>\n",
       "      <td>1.202317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>beckham</td>\n",
       "      <td>1.168514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>ant-man</td>\n",
       "      <td>1.159696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>forward</td>\n",
       "      <td>1.154021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>impressive</td>\n",
       "      <td>1.126998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>ihop</td>\n",
       "      <td>1.126029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>madonna</td>\n",
       "      <td>1.125788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>loved</td>\n",
       "      <td>1.123043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>ric</td>\n",
       "      <td>1.115740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>sting</td>\n",
       "      <td>1.107526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18632</th>\n",
       "      <td>problem</td>\n",
       "      <td>-1.350576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18633</th>\n",
       "      <td>fucked</td>\n",
       "      <td>-1.358650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18634</th>\n",
       "      <td>ruined</td>\n",
       "      <td>-1.376086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18635</th>\n",
       "      <td>erdogan</td>\n",
       "      <td>-1.391461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18636</th>\n",
       "      <td>loss</td>\n",
       "      <td>-1.409846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18637</th>\n",
       "      <td>die</td>\n",
       "      <td>-1.434337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18638</th>\n",
       "      <td>bad</td>\n",
       "      <td>-1.435419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18639</th>\n",
       "      <td>muslims</td>\n",
       "      <td>-1.439173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18640</th>\n",
       "      <td>hell</td>\n",
       "      <td>-1.446053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18641</th>\n",
       "      <td>missing</td>\n",
       "      <td>-1.484901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18642</th>\n",
       "      <td>sucks</td>\n",
       "      <td>-1.489892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18643</th>\n",
       "      <td>terrible</td>\n",
       "      <td>-1.504740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18644</th>\n",
       "      <td>disappointed</td>\n",
       "      <td>-1.510845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18645</th>\n",
       "      <td>died</td>\n",
       "      <td>-1.544923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18646</th>\n",
       "      <td>smh</td>\n",
       "      <td>-1.569413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18647</th>\n",
       "      <td>haven't</td>\n",
       "      <td>-1.599412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18648</th>\n",
       "      <td>jealous</td>\n",
       "      <td>-1.603778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18649</th>\n",
       "      <td>ira</td>\n",
       "      <td>-1.616393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18650</th>\n",
       "      <td>stupid</td>\n",
       "      <td>-1.633989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18651</th>\n",
       "      <td>lost</td>\n",
       "      <td>-1.701476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18652</th>\n",
       "      <td>shit</td>\n",
       "      <td>-1.707183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18653</th>\n",
       "      <td>damn</td>\n",
       "      <td>-1.728773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18654</th>\n",
       "      <td>yakub</td>\n",
       "      <td>-1.773558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18655</th>\n",
       "      <td>sad</td>\n",
       "      <td>-1.794724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18656</th>\n",
       "      <td>#sarcasm</td>\n",
       "      <td>-1.997757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18657</th>\n",
       "      <td>worse</td>\n",
       "      <td>-2.001194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18658</th>\n",
       "      <td>hate</td>\n",
       "      <td>-2.071635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18659</th>\n",
       "      <td>:(</td>\n",
       "      <td>-2.269209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18660</th>\n",
       "      <td>fuck</td>\n",
       "      <td>-2.348031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18661</th>\n",
       "      <td>worst</td>\n",
       "      <td>-2.847381</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18662 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               word     coeff\n",
       "0              love  2.222775\n",
       "1              best  2.196226\n",
       "2           awesome  2.115709\n",
       "3                :)  2.049287\n",
       "4           amazing  1.993686\n",
       "5               fun  1.703092\n",
       "6           excited  1.623278\n",
       "7             happy  1.547635\n",
       "8              nice  1.499429\n",
       "9         beautiful  1.487371\n",
       "10            thank  1.486662\n",
       "11             good  1.408511\n",
       "12            great  1.360709\n",
       "13          perfect  1.324802\n",
       "14             cool  1.286487\n",
       "15      interesting  1.281344\n",
       "16         november  1.260746\n",
       "17              won  1.247853\n",
       "18         december  1.234652\n",
       "19        brilliant  1.213485\n",
       "20             holy  1.202317\n",
       "21          beckham  1.168514\n",
       "22          ant-man  1.159696\n",
       "23          forward  1.154021\n",
       "24       impressive  1.126998\n",
       "25             ihop  1.126029\n",
       "26          madonna  1.125788\n",
       "27            loved  1.123043\n",
       "28              ric  1.115740\n",
       "29            sting  1.107526\n",
       "...             ...       ...\n",
       "18632       problem -1.350576\n",
       "18633        fucked -1.358650\n",
       "18634        ruined -1.376086\n",
       "18635       erdogan -1.391461\n",
       "18636          loss -1.409846\n",
       "18637           die -1.434337\n",
       "18638           bad -1.435419\n",
       "18639       muslims -1.439173\n",
       "18640          hell -1.446053\n",
       "18641       missing -1.484901\n",
       "18642         sucks -1.489892\n",
       "18643      terrible -1.504740\n",
       "18644  disappointed -1.510845\n",
       "18645          died -1.544923\n",
       "18646           smh -1.569413\n",
       "18647       haven't -1.599412\n",
       "18648       jealous -1.603778\n",
       "18649           ira -1.616393\n",
       "18650        stupid -1.633989\n",
       "18651          lost -1.701476\n",
       "18652          shit -1.707183\n",
       "18653          damn -1.728773\n",
       "18654         yakub -1.773558\n",
       "18655           sad -1.794724\n",
       "18656      #sarcasm -1.997757\n",
       "18657         worse -2.001194\n",
       "18658          hate -2.071635\n",
       "18659            :( -2.269209\n",
       "18660          fuck -2.348031\n",
       "18661         worst -2.847381\n",
       "\n",
       "[18662 rows x 2 columns]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_names = first_pipeline.steps[0][1].get_feature_names()\n",
    "feature_coeffs = first_pipeline.steps[1][1].coef_\n",
    "\n",
    "feature_coeffs = pd.DataFrame(list(zip(feature_names,feature_coeffs.reshape((-1)))), \n",
    "                              columns = [\"word\",\"coeff\"])\n",
    "feature_coeffs = feature_coeffs.sort_values(by = \"coeff\",\n",
    "                                            ascending = False).reset_index(drop = True)\n",
    " \n",
    "feature_coeffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('countVect',\n",
       "  CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "          dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "          lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "          ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "          strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "          tokenizer=<bound method TweetTokenizer.tokenize of <nltk.tokenize.casual.TweetTokenizer object at 0x1207bbf98>>,\n",
       "          vocabulary=None)),\n",
       " ('lr',\n",
       "  LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "            intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "            penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "            verbose=0, warm_start=False))]"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_pipeline.steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets create a function that accepts a vectorizer and returns a table with the coefficients and accuracy of cv-ed model\n",
    "def tokenize_test(vect,clf):\n",
    "    pipe = Pipeline([(\"vect\",vect),(\"lr\",clf)])\n",
    "    pipe.fit(semeval_data.tweet,semeval_data.target)\n",
    "    num_features = len(pipe.steps[0][1].get_feature_names())\n",
    "    print('Num Features: ', num_features)\n",
    "\n",
    "    zipped_coeffs = list(zip(pipe.steps[0][1].get_feature_names(),\n",
    "                             pipe.steps[1][1].coef_[0]))\n",
    "    feature_coeffs = pd.DataFrame(zipped_coeffs,columns=[\"word\",\"coeff\"]).sort_values(by=\"coeff\",ascending=False)\n",
    "    feature_coeffs.reset_index(drop=True,inplace=True)\n",
    "\n",
    "    strat_cv = StratifiedKFold(n_splits=10)\n",
    "    acc = np.mean(cross_val_score(pipe,\n",
    "                                  semeval_data.tweet,\n",
    "                                  semeval_data.target,\n",
    "                                  scoring=\"accuracy\",\n",
    "                                  cv=strat_cv,\n",
    "                                  n_jobs=-1,\n",
    "                                  verbose=1))\n",
    "    print(\"Accuracy:\", acc)\n",
    "    return (feature_coeffs, acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Features:  106388\n",
      "Accuracy: 0.8066137754798837\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:   10.5s finished\n"
     ]
    }
   ],
   "source": [
    "# include 1-grams and 2-grams\n",
    "vect = CountVectorizer(tokenizer=tokenizer_for_tweets.tokenize,\n",
    "                       ngram_range=(1,2))\n",
    "feature_coeffs,acc = tokenize_test(vect,LogisticRegression())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>coeff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>love</td>\n",
       "      <td>2.099413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>best</td>\n",
       "      <td>1.872566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>:)</td>\n",
       "      <td>1.783731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>happy</td>\n",
       "      <td>1.534225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>awesome</td>\n",
       "      <td>1.502071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>good</td>\n",
       "      <td>1.377432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>amazing</td>\n",
       "      <td>1.247889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>fun</td>\n",
       "      <td>1.211655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>excited</td>\n",
       "      <td>1.196532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>great</td>\n",
       "      <td>1.154848</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      word     coeff\n",
       "0     love  2.099413\n",
       "1     best  1.872566\n",
       "2       :)  1.783731\n",
       "3    happy  1.534225\n",
       "4  awesome  1.502071\n",
       "5     good  1.377432\n",
       "6  amazing  1.247889\n",
       "7      fun  1.211655\n",
       "8  excited  1.196532\n",
       "9    great  1.154848"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_coeffs.head(10) # most positive words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>coeff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>106387</th>\n",
       "      <td>fuck</td>\n",
       "      <td>-1.967757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106386</th>\n",
       "      <td>:(</td>\n",
       "      <td>-1.754297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106385</th>\n",
       "      <td>worst</td>\n",
       "      <td>-1.659690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106384</th>\n",
       "      <td>shit</td>\n",
       "      <td>-1.538234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106383</th>\n",
       "      <td>hate</td>\n",
       "      <td>-1.463272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106382</th>\n",
       "      <td>bad</td>\n",
       "      <td>-1.443930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106381</th>\n",
       "      <td>#sarcasm</td>\n",
       "      <td>-1.406891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106380</th>\n",
       "      <td>don't</td>\n",
       "      <td>-1.354917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106379</th>\n",
       "      <td>sad</td>\n",
       "      <td>-1.352075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106378</th>\n",
       "      <td>yakub</td>\n",
       "      <td>-1.328025</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            word     coeff\n",
       "106387      fuck -1.967757\n",
       "106386        :( -1.754297\n",
       "106385     worst -1.659690\n",
       "106384      shit -1.538234\n",
       "106383      hate -1.463272\n",
       "106382       bad -1.443930\n",
       "106381  #sarcasm -1.406891\n",
       "106380     don't -1.354917\n",
       "106379       sad -1.352075\n",
       "106378     yakub -1.328025"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_coeffs.sort_values(\"coeff\").head(10) # most negative words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_coeffs[\"abs_coeffs\"] = feature_coeffs.coeff.abs() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>coeff</th>\n",
       "      <th>abs_coeffs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>love</td>\n",
       "      <td>2.099413</td>\n",
       "      <td>2.099413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106387</th>\n",
       "      <td>fuck</td>\n",
       "      <td>-1.967757</td>\n",
       "      <td>1.967757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>best</td>\n",
       "      <td>1.872566</td>\n",
       "      <td>1.872566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>:)</td>\n",
       "      <td>1.783731</td>\n",
       "      <td>1.783731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106386</th>\n",
       "      <td>:(</td>\n",
       "      <td>-1.754297</td>\n",
       "      <td>1.754297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106385</th>\n",
       "      <td>worst</td>\n",
       "      <td>-1.659690</td>\n",
       "      <td>1.659690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106384</th>\n",
       "      <td>shit</td>\n",
       "      <td>-1.538234</td>\n",
       "      <td>1.538234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>happy</td>\n",
       "      <td>1.534225</td>\n",
       "      <td>1.534225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>awesome</td>\n",
       "      <td>1.502071</td>\n",
       "      <td>1.502071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106383</th>\n",
       "      <td>hate</td>\n",
       "      <td>-1.463272</td>\n",
       "      <td>1.463272</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           word     coeff  abs_coeffs\n",
       "0          love  2.099413    2.099413\n",
       "106387     fuck -1.967757    1.967757\n",
       "1          best  1.872566    1.872566\n",
       "2            :)  1.783731    1.783731\n",
       "106386       :( -1.754297    1.754297\n",
       "106385    worst -1.659690    1.659690\n",
       "106384     shit -1.538234    1.538234\n",
       "3         happy  1.534225    1.534225\n",
       "4       awesome  1.502071    1.502071\n",
       "106383     hate -1.463272    1.463272"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_coeffs.sort_values(\"abs_coeffs\",ascending=False).head(10) # absolute values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stopword Removal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **What:** Remove common words that will likely appear in any text\n",
    "- **Why:** They don't tell you much about your text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 2), preprocessor=None, stop_words=None,\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=<bound method TweetTokenizer.tokenize of <nltk.tokenize.casual.TweetTokenizer object at 0x1a134644a8>>,\n",
       "        vocabulary=None)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show vectorizer options\n",
    "vect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **stop_words:** `string` {`'english'`}, `list`, or `None` (default)\n",
    "- If `'english'`, a built-in stop word list for English is used.\n",
    "- If a `list`, that list is assumed to contain stop words, all of which will be removed from the resulting tokens.\n",
    "- If `None`, no stop words will be used. \n",
    "- `max_df` can be set to a value in the range [0.7, 1.0) to automatically detect and filter stop words based on within-corpus document frequency of terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Features:  18379\n",
      "Accuracy: 0.8090057645450763\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:    4.7s finished\n"
     ]
    }
   ],
   "source": [
    "# remove English stop words\n",
    "vect = CountVectorizer(stop_words = 'english',\n",
    "                       tokenizer = tokenizer_for_tweets.tokenize)\n",
    "\n",
    "feature_coeffs,acc = tokenize_test(vect,\n",
    "                                   LogisticRegression())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>coeff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>love</td>\n",
       "      <td>2.255828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>:)</td>\n",
       "      <td>2.184607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>best</td>\n",
       "      <td>2.129766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>awesome</td>\n",
       "      <td>2.010117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>amazing</td>\n",
       "      <td>1.816643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>thank</td>\n",
       "      <td>1.799106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>fun</td>\n",
       "      <td>1.653984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>nice</td>\n",
       "      <td>1.537520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>excited</td>\n",
       "      <td>1.488616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>beautiful</td>\n",
       "      <td>1.420194</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        word     coeff\n",
       "0       love  2.255828\n",
       "1         :)  2.184607\n",
       "2       best  2.129766\n",
       "3    awesome  2.010117\n",
       "4    amazing  1.816643\n",
       "5      thank  1.799106\n",
       "6        fun  1.653984\n",
       "7       nice  1.537520\n",
       "8    excited  1.488616\n",
       "9  beautiful  1.420194"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_coeffs.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>coeff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>18369</th>\n",
       "      <td>lost</td>\n",
       "      <td>-1.809094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18370</th>\n",
       "      <td>sad</td>\n",
       "      <td>-1.840014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18371</th>\n",
       "      <td>shit</td>\n",
       "      <td>-1.881805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18372</th>\n",
       "      <td>yakub</td>\n",
       "      <td>-1.913493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18373</th>\n",
       "      <td>hate</td>\n",
       "      <td>-1.938413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18374</th>\n",
       "      <td>#sarcasm</td>\n",
       "      <td>-2.085188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18375</th>\n",
       "      <td>worse</td>\n",
       "      <td>-2.129059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18376</th>\n",
       "      <td>fuck</td>\n",
       "      <td>-2.204128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18377</th>\n",
       "      <td>:(</td>\n",
       "      <td>-2.248473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18378</th>\n",
       "      <td>worst</td>\n",
       "      <td>-2.718607</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           word     coeff\n",
       "18369      lost -1.809094\n",
       "18370       sad -1.840014\n",
       "18371      shit -1.881805\n",
       "18372     yakub -1.913493\n",
       "18373      hate -1.938413\n",
       "18374  #sarcasm -2.085188\n",
       "18375     worse -2.129059\n",
       "18376      fuck -2.204128\n",
       "18377        :( -2.248473\n",
       "18378     worst -2.718607"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_coeffs.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>coeff</th>\n",
       "      <th>abs_coeffs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9189</th>\n",
       "      <td>6357677984</td>\n",
       "      <td>-0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9087</th>\n",
       "      <td>4675456</td>\n",
       "      <td>-0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9086</th>\n",
       "      <td>forms</td>\n",
       "      <td>-0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9085</th>\n",
       "      <td>http://t.co/0ai2qibbcr</td>\n",
       "      <td>-0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9084</th>\n",
       "      <td>5117158</td>\n",
       "      <td>-0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9083</th>\n",
       "      <td>5190111961</td>\n",
       "      <td>-0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9082</th>\n",
       "      <td>5204224</td>\n",
       "      <td>-0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9081</th>\n",
       "      <td>7116006</td>\n",
       "      <td>-0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9080</th>\n",
       "      <td>4488678</td>\n",
       "      <td>-0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9079</th>\n",
       "      <td>5227747634</td>\n",
       "      <td>-0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        word     coeff  abs_coeffs\n",
       "9189              6357677984 -0.000001    0.000001\n",
       "9087                 4675456 -0.000001    0.000001\n",
       "9086                   forms -0.000001    0.000001\n",
       "9085  http://t.co/0ai2qibbcr -0.000001    0.000001\n",
       "9084                 5117158 -0.000001    0.000001\n",
       "9083              5190111961 -0.000001    0.000001\n",
       "9082                 5204224 -0.000001    0.000001\n",
       "9081                 7116006 -0.000001    0.000001\n",
       "9080                 4488678 -0.000001    0.000001\n",
       "9079              5227747634 -0.000001    0.000001"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_coeffs[\"abs_coeffs\"] = feature_coeffs.coeff.abs()\n",
    "feature_coeffs.sort_values(\"abs_coeffs\").head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frozenset({'others', 'each', 'perhaps', 'sixty', 'against', 'or', 'mill', 'de', 'via', 'off', 'get', 'am', 'fire', 'always', 'amoungst', 'three', 'once', 'found', 'in', 'neither', 'become', 'nobody', 'yet', 'any', 'can', 'find', 'about', 'an', 'namely', 'before', 'i', 'they', 'otherwise', 'please', 'and', 'cannot', 'every', 'his', 'me', 'often', 'onto', 'below', 'which', 'other', 'these', 'been', 'eight', 'him', 'at', 'even', 'so', 'already', 'well', 'again', 'it', 'eg', 'for', 'amongst', 'sometime', 'take', 'whither', 'here', 'beforehand', 'else', 'then', 'who', 'whether', 'might', 'that', 'be', 'has', 'this', 'less', 'much', 'whereafter', 'wherein', 'yours', 'hers', 'how', 'seemed', 'while', 'sometimes', 'third', 'themselves', 'move', 'fifty', 'itself', 'although', 'should', 'too', 'himself', 'there', 'them', 'among', 'than', 'after', 'through', 'but', 'whereupon', 'fill', 'with', 'con', 'beyond', 'side', 'almost', 'also', 'hereafter', 'un', 'becomes', 'have', 'our', 'toward', 'we', 'therein', 'towards', 'nothing', 'whereby', 'mostly', 'nine', 'hence', 'beside', 'six', 'some', 'until', 'along', 'top', 'everyone', 'ie', 'etc', 'inc', 'hundred', 'as', 'enough', 'by', 'nevertheless', 'except', 'she', 'of', 'describe', 'eleven', 'system', 'would', 'anything', 'your', 'though', 'nor', 'somewhere', 'whatever', 'within', 'thick', 'no', 'will', 'seems', 'must', 'done', 'their', 'anyway', 'across', 'over', 'cry', 'anywhere', 'hasnt', 'on', 'fifteen', 'thereafter', 'last', 'latter', 'due', 'may', 'more', 'give', 're', 'between', 'never', 'anyone', 'bill', 'empty', 'around', 'bottom', 'latterly', 'not', 'those', 'whoever', 'elsewhere', 'besides', 'however', 'something', 'made', 'whereas', 'keep', 'down', 'alone', 'detail', 'why', 'rather', 'same', 'four', 'ours', 'yourselves', 'afterwards', 'what', 'someone', 'twelve', 'most', 'only', 'without', 'above', 'interest', 'twenty', 'hereupon', 'further', 'being', 'had', 'us', 'if', 'noone', 'sincere', 'up', 'all', 'seeming', 'go', 'another', 'thereby', 'when', 'myself', 'ltd', 'mine', 'became', 'none', 'during', 'either', 'first', 'were', 'her', 'could', 'to', 'such', 'behind', 'put', 'very', 'thin', 'because', 'herself', 'thence', 'many', 'together', 'whence', 'forty', 'two', 'out', 'whole', 'five', 'least', 'now', 'thereupon', 'one', 'its', 'was', 'where', 'front', 'somehow', 'cant', 'upon', 'meanwhile', 'yourself', 'show', 'are', 'do', 'whom', 'several', 'hereby', 'whose', 'still', 'ourselves', 'former', 'he', 'my', 'see', 'moreover', 'ever', 'thru', 'wherever', 'from', 'call', 'since', 'everywhere', 'back', 'part', 'anyhow', 'next', 'co', 'ten', 'amount', 'you', 'seem', 'everything', 'is', 'a', 'the', 'formerly', 'serious', 'becoming', 'into', 'nowhere', 'thus', 'couldnt', 'per', 'under', 'whenever', 'both', 'name', 'indeed', 'throughout', 'full', 'therefore', 'few', 'herein', 'own'})\n"
     ]
    }
   ],
   "source": [
    "# set of stop words\n",
    "print(vect.get_stop_words())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other CountVectorizer Options"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **max_features:** int or None, default=None\n",
    "- If not None, build a vocabulary that only consider the top max_features ordered by term frequency across the corpus.\n",
    "\n",
    "- **min_df:** float in range [0.0, 1.0] or int, default=1\n",
    "- When building the vocabulary ignore terms that have a document frequency strictly lower than the given threshold. This value is also called cut-off in the literature. If float, the parameter represents a proportion of documents, integer absolute counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Features:  376\n",
      "Accuracy: 0.7449664230106376\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:    4.6s finished\n"
     ]
    }
   ],
   "source": [
    "# remove English stop words and only keep words appearing in 0.5% of documents\n",
    "#and never appear in more than 70% of documents\n",
    "vect = CountVectorizer(stop_words ='english', \n",
    "                       min_df = 0.005, \n",
    "                       max_df = 0.7,\n",
    "                       tokenizer = tokenizer_for_tweets.tokenize)\n",
    "\n",
    "features,acc = tokenize_test(vect,\n",
    "                             LogisticRegression())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>coeff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>love</td>\n",
       "      <td>2.309081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>:)</td>\n",
       "      <td>2.175858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>awesome</td>\n",
       "      <td>2.144141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>amazing</td>\n",
       "      <td>2.101960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>best</td>\n",
       "      <td>1.855652</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      word     coeff\n",
       "0     love  2.309081\n",
       "1       :)  2.175858\n",
       "2  awesome  2.144141\n",
       "3  amazing  2.101960\n",
       "4     best  1.855652"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>coeff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>381</th>\n",
       "      <td>worst</td>\n",
       "      <td>-2.813687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>380</th>\n",
       "      <td>yakub</td>\n",
       "      <td>-2.559607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>379</th>\n",
       "      <td>ira</td>\n",
       "      <td>-2.240680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>378</th>\n",
       "      <td>fuck</td>\n",
       "      <td>-2.166876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>377</th>\n",
       "      <td>:(</td>\n",
       "      <td>-2.142507</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      word     coeff\n",
       "381  worst -2.813687\n",
       "380  yakub -2.559607\n",
       "379    ira -2.240680\n",
       "378   fuck -2.166876\n",
       "377     :( -2.142507"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.sort_values(\"coeff\").head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Features:  1463\n",
      "Accuracy: 0.7791654285770884\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:    8.0s finished\n"
     ]
    }
   ],
   "source": [
    "# include 1-grams and 2-grams, and only include terms that appear at least 20 times\n",
    "vect = CountVectorizer(ngram_range=(1, 2), \n",
    "                       min_df=20,\n",
    "                       tokenizer = tokenizer_for_tweets.tokenize)\n",
    "\n",
    "features,acc = tokenize_test(vect,LogisticRegression())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>coeff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>awesome</td>\n",
       "      <td>2.542365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>love</td>\n",
       "      <td>2.385760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>amazing</td>\n",
       "      <td>2.347981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>:)</td>\n",
       "      <td>2.176935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>can't wait</td>\n",
       "      <td>2.051356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>best</td>\n",
       "      <td>2.039828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>fun</td>\n",
       "      <td>2.006537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>beautiful</td>\n",
       "      <td>1.810813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>happy</td>\n",
       "      <td>1.803836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>nice</td>\n",
       "      <td>1.670451</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         word     coeff\n",
       "0     awesome  2.542365\n",
       "1        love  2.385760\n",
       "2     amazing  2.347981\n",
       "3          :)  2.176935\n",
       "4  can't wait  2.051356\n",
       "5        best  2.039828\n",
       "6         fun  2.006537\n",
       "7   beautiful  1.810813\n",
       "8       happy  1.803836\n",
       "9        nice  1.670451"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>coeff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1462</th>\n",
       "      <td>worst</td>\n",
       "      <td>-2.527061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1461</th>\n",
       "      <td>fuck</td>\n",
       "      <td>-2.481634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1460</th>\n",
       "      <td>hate</td>\n",
       "      <td>-2.214171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1459</th>\n",
       "      <td>#sarcasm</td>\n",
       "      <td>-2.193800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1458</th>\n",
       "      <td>yakub</td>\n",
       "      <td>-2.171891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1457</th>\n",
       "      <td>:(</td>\n",
       "      <td>-2.153027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1456</th>\n",
       "      <td>worse</td>\n",
       "      <td>-2.135446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1455</th>\n",
       "      <td>loss</td>\n",
       "      <td>-2.106426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1454</th>\n",
       "      <td>stupid</td>\n",
       "      <td>-2.056276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1453</th>\n",
       "      <td>died</td>\n",
       "      <td>-2.054732</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          word     coeff\n",
       "1462     worst -2.527061\n",
       "1461      fuck -2.481634\n",
       "1460      hate -2.214171\n",
       "1459  #sarcasm -2.193800\n",
       "1458     yakub -2.171891\n",
       "1457        :( -2.153027\n",
       "1456     worse -2.135446\n",
       "1455      loss -2.106426\n",
       "1454    stupid -2.056276\n",
       "1453      died -2.054732"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.sort_values(\"coeff\").head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>coeff</th>\n",
       "      <th>abs_coeffs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>797</th>\n",
       "      <td>cube</td>\n",
       "      <td>-0.000546</td>\n",
       "      <td>0.000546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>798</th>\n",
       "      <td>ice cube</td>\n",
       "      <td>-0.000546</td>\n",
       "      <td>0.000546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>799</th>\n",
       "      <td>dec</td>\n",
       "      <td>-0.001517</td>\n",
       "      <td>0.001517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>800</th>\n",
       "      <td>update</td>\n",
       "      <td>-0.001527</td>\n",
       "      <td>0.001527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>796</th>\n",
       "      <td>. what</td>\n",
       "      <td>0.001625</td>\n",
       "      <td>0.001625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>801</th>\n",
       "      <td>else</td>\n",
       "      <td>-0.001840</td>\n",
       "      <td>0.001840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>795</th>\n",
       "      <td>off to</td>\n",
       "      <td>0.002268</td>\n",
       "      <td>0.002268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>802</th>\n",
       "      <td>need to</td>\n",
       "      <td>-0.003526</td>\n",
       "      <td>0.003526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>803</th>\n",
       "      <td>but it</td>\n",
       "      <td>-0.004337</td>\n",
       "      <td>0.004337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>804</th>\n",
       "      <td>game</td>\n",
       "      <td>-0.004619</td>\n",
       "      <td>0.004619</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         word     coeff  abs_coeffs\n",
       "797      cube -0.000546    0.000546\n",
       "798  ice cube -0.000546    0.000546\n",
       "799       dec -0.001517    0.001517\n",
       "800    update -0.001527    0.001527\n",
       "796    . what  0.001625    0.001625\n",
       "801      else -0.001840    0.001840\n",
       "795    off to  0.002268    0.002268\n",
       "802   need to -0.003526    0.003526\n",
       "803    but it -0.004337    0.004337\n",
       "804      game -0.004619    0.004619"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features[\"abs_coeffs\"] = features.coeff.abs()\n",
    "features.sort_values(\"abs_coeffs\").head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Term Frequency - Inverse Document Frequency (TF-IDF) Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **What:** Computes \"relative frequency\" that a word appears in a document compared to its frequency across all documents\n",
    "- **Why:** More useful than \"term frequency\" for identifying \"important\" words in each document \n",
    "    - high frequency in that document, low frequency in other documents\n",
    "- **Notes:** Used for search engine scoring, text summarization, document clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example documents\n",
    "train_simple = ['call you tonight',\n",
    "                'Call me a cab',\n",
    "                'please call me... PLEASE!']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cab</th>\n",
       "      <th>call</th>\n",
       "      <th>me</th>\n",
       "      <th>please</th>\n",
       "      <th>tonight</th>\n",
       "      <th>you</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cab  call  me  please  tonight  you\n",
       "0    0     1   0       0        1    1\n",
       "1    1     1   1       0        0    0\n",
       "2    0     1   1       2        0    0"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CountVectorizer\n",
    "vect = CountVectorizer()\n",
    "\n",
    "pd.DataFrame(vect.fit_transform(train_simple).toarray(), \n",
    "             columns = vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cab</th>\n",
       "      <th>call</th>\n",
       "      <th>me</th>\n",
       "      <th>please</th>\n",
       "      <th>tonight</th>\n",
       "      <th>you</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.385372</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.652491</td>\n",
       "      <td>0.652491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.720333</td>\n",
       "      <td>0.425441</td>\n",
       "      <td>0.547832</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.266075</td>\n",
       "      <td>0.342620</td>\n",
       "      <td>0.901008</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        cab      call        me    please   tonight       you\n",
       "0  0.000000  0.385372  0.000000  0.000000  0.652491  0.652491\n",
       "1  0.720333  0.425441  0.547832  0.000000  0.000000  0.000000\n",
       "2  0.000000  0.266075  0.342620  0.901008  0.000000  0.000000"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TfidfVectorizer\n",
    "vect = TfidfVectorizer()\n",
    "pd.DataFrame(vect.fit_transform(train_simple).toarray(), \n",
    "             columns = vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Features:  18662\n",
      "Accuracy: 0.8033779045581506\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:    5.0s finished\n"
     ]
    }
   ],
   "source": [
    "vect = TfidfVectorizer(tokenizer = tokenizer_for_tweets.tokenize)\n",
    "tfidf_coeffs,acc = tokenize_test(vect,LogisticRegression())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Features:  1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:    5.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7866242744795071\n",
      "Num Features:  10000\n",
      "Accuracy: 0.8040819318159308\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:    5.8s finished\n"
     ]
    }
   ],
   "source": [
    "for max_features in (1000,10000):\n",
    "    vect = TfidfVectorizer(max_features = max_features,\n",
    "                           tokenizer = tokenizer_for_tweets.tokenize)\n",
    "    tokenize_test(vect,LogisticRegression())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Features:  10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:    8.4s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7990142826013749\n",
      "Num Features:  10000\n",
      "Accuracy: 0.7987351676868524\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:   11.0s finished\n"
     ]
    }
   ],
   "source": [
    "for ngram_range in (2,3):\n",
    "    vect = TfidfVectorizer(max_features = 10000,\n",
    "                           ngram_range = (1,ngram_range),\n",
    "                           tokenizer = tokenizer_for_tweets.tokenize)\n",
    "    tokenize_test(vect,LogisticRegression())    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>coeff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>love</td>\n",
       "      <td>4.555348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>!</td>\n",
       "      <td>4.508514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>best</td>\n",
       "      <td>4.404152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>good</td>\n",
       "      <td>3.577530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>:)</td>\n",
       "      <td>3.367341</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   word     coeff\n",
       "0  love  4.555348\n",
       "1     !  4.508514\n",
       "2  best  4.404152\n",
       "3  good  3.577530\n",
       "4    :)  3.367341"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_coeffs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>coeff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20754</th>\n",
       "      <td>not</td>\n",
       "      <td>-3.960406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20753</th>\n",
       "      <td>fuck</td>\n",
       "      <td>-3.484320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20752</th>\n",
       "      <td>don't</td>\n",
       "      <td>-3.222726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20751</th>\n",
       "      <td>:(</td>\n",
       "      <td>-3.003267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20750</th>\n",
       "      <td>no</td>\n",
       "      <td>-2.975694</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        word     coeff\n",
       "20754    not -3.960406\n",
       "20753   fuck -3.484320\n",
       "20752  don't -3.222726\n",
       "20751     :( -3.003267\n",
       "20750     no -2.975694"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_coeffs.sort_values(\"coeff\").head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>coeff</th>\n",
       "      <th>abs_coeffs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10610</th>\n",
       "      <td>blade</td>\n",
       "      <td>-0.000037</td>\n",
       "      <td>0.000037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10611</th>\n",
       "      <td>quest</td>\n",
       "      <td>-0.000080</td>\n",
       "      <td>0.000080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10609</th>\n",
       "      <td>name</td>\n",
       "      <td>0.000092</td>\n",
       "      <td>0.000092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10608</th>\n",
       "      <td>@chrislhayes</td>\n",
       "      <td>0.000101</td>\n",
       "      <td>0.000101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10612</th>\n",
       "      <td>islamist</td>\n",
       "      <td>-0.000110</td>\n",
       "      <td>0.000110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10607</th>\n",
       "      <td>press</td>\n",
       "      <td>0.000279</td>\n",
       "      <td>0.000279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10613</th>\n",
       "      <td>taker</td>\n",
       "      <td>-0.000328</td>\n",
       "      <td>0.000328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10614</th>\n",
       "      <td>imma</td>\n",
       "      <td>-0.000329</td>\n",
       "      <td>0.000329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10615</th>\n",
       "      <td>all</td>\n",
       "      <td>-0.000464</td>\n",
       "      <td>0.000464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10606</th>\n",
       "      <td>feels</td>\n",
       "      <td>0.000491</td>\n",
       "      <td>0.000491</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               word     coeff  abs_coeffs\n",
       "10610         blade -0.000037    0.000037\n",
       "10611         quest -0.000080    0.000080\n",
       "10609          name  0.000092    0.000092\n",
       "10608  @chrislhayes  0.000101    0.000101\n",
       "10612      islamist -0.000110    0.000110\n",
       "10607         press  0.000279    0.000279\n",
       "10613         taker -0.000328    0.000328\n",
       "10614          imma -0.000329    0.000329\n",
       "10615           all -0.000464    0.000464\n",
       "10606         feels  0.000491    0.000491"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_coeffs[\"abs_coeffs\"] = tfidf_coeffs.coeff.abs()\n",
    "tfidf_coeffs.sort_values(\"abs_coeffs\").head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Introduction to TextBlob](http://textblob.readthedocs.io/en/dev/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'I wish I was in Bolton tonight :('\n"
     ]
    }
   ],
   "source": [
    "# print the first tweet\n",
    "print(semeval_data.tweet.values[20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet = textblob.TextBlob(semeval_data.tweet.values[20].decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the command below fails, run `conda install -c conda-forge textblob ` from a terminal window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/suesong/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "WordList([u'I', u'wish', u'I', u'was', u'in', u'Bolton', u'tonight'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"punkt\")\n",
    "# list the words\n",
    "tweet.words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Sentence(\"I wish I was in Bolton tonight :(\")]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# list the sentences\n",
    "tweet.sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextBlob(\"i wish i was in bolton tonight :(\")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# some string methods are available\n",
    "tweet.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming and Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Stemming:**\n",
    "\n",
    "- **What:** Reduce a word to its base/stem/root form\n",
    "- **Why:** Often makes sense to treat related words the same way\n",
    "- **Notes:**\n",
    "    - Uses a \"simple\" and fast rule-based approach\n",
    "    - Stemmed words are usually not shown to users (used for analysis/indexing)\n",
    "    - Some search engines treat words with the same stem as synonyms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the import below fails, run `conda install -c conda-forge nltk` in a terminal window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'i', u'wish', u'i', u'was', u'in', u'bolton', u'tonight']\n"
     ]
    }
   ],
   "source": [
    "# initialize stemmer\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "stemmer = SnowballStemmer('english')\n",
    "\n",
    "# stem each word\n",
    "print([stemmer.stem(word) for word in tweet.words])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lemmatization**\n",
    "\n",
    "- **What:** Derive the canonical form ('lemma') of a word\n",
    "- **Why:** Can be better than stemming\n",
    "- **Notes:** Uses a dictionary-based approach (slower than stemming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/suesong/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[u'I', u'wish', u'I', u'wa', u'in', u'Bolton', u'tonight']\n"
     ]
    }
   ],
   "source": [
    "nltk.download(\"wordnet\")\n",
    "# assume every word is a noun\n",
    "print([word.lemmatize() for word in tweet.words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'I', u'wish', u'I', u'be', u'in', u'Bolton', u'tonight']\n"
     ]
    }
   ],
   "source": [
    "# assume every word is a verb\n",
    "print([word.lemmatize(pos='v') for word in tweet.words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function that accepts text and returns a list of lemmas\n",
    "def split_into_lemmas(text):\n",
    "    text = text.lower()\n",
    "    words = textblob.TextBlob(text.decode(\"utf-8\")).words\n",
    "    return [word.lemmatize() for word in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenize_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-dd01dce00a33>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# use split_into_lemmas as the feature extraction function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mvect\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manalyzer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msplit_into_lemmas\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenize_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvect\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mLogisticRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'tokenize_test' is not defined"
     ]
    }
   ],
   "source": [
    "# use split_into_lemmas as the feature extraction function\n",
    "vect = CountVectorizer(analyzer=split_into_lemmas)\n",
    "features,acc = tokenize_test(vect,LogisticRegression())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>coeff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>awesome</td>\n",
       "      <td>2.430680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>love</td>\n",
       "      <td>2.268086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>best</td>\n",
       "      <td>2.183874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>amazing</td>\n",
       "      <td>2.010210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fun</td>\n",
       "      <td>1.838472</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      word     coeff\n",
       "0  awesome  2.430680\n",
       "1     love  2.268086\n",
       "2     best  2.183874\n",
       "3  amazing  2.010210\n",
       "4      fun  1.838472"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>coeff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19163</th>\n",
       "      <td>worst</td>\n",
       "      <td>-2.798196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19162</th>\n",
       "      <td>fuck</td>\n",
       "      <td>-2.374917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19161</th>\n",
       "      <td>hate</td>\n",
       "      <td>-2.234440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19160</th>\n",
       "      <td>worse</td>\n",
       "      <td>-2.005715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19159</th>\n",
       "      <td>sad</td>\n",
       "      <td>-1.920527</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        word     coeff\n",
       "19163  worst -2.798196\n",
       "19162   fuck -2.374917\n",
       "19161   hate -2.234440\n",
       "19160  worse -2.005715\n",
       "19159    sad -1.920527"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.sort_values(\"coeff\").head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis in TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I wish I was in Bolton tonight :(\n"
     ]
    }
   ],
   "source": [
    "print(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentiment(polarity=-0.75, subjectivity=1.0)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# polarity ranges from -1 (most negative) to 1 (most positive)\n",
    "tweet.sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "semeval_data['tweet_length'] = semeval_data.tweet.apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function that accepts text and returns the polarity\n",
    "def detect_sentiment(text):\n",
    "    return textblob.TextBlob(text.decode(\"utf-8\")).sentiment.polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new DataFrame column for sentiment\n",
    "semeval_data['textblob_sentiment'] = semeval_data.tweet.apply(detect_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1a15d1e990>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# boxplot of sentiment grouped by stars\n",
    "semeval_data.boxplot(column='textblob_sentiment', by='sentiment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "72     it's supposed to snow from midnight tonight un...\n",
       "122    @KylieJenner take a pic with @britneyspears on...\n",
       "216    Now that Iran ripped us off by the legendary W...\n",
       "399    Yup. Great quote @StefanMolyneux : \"He who is ...\n",
       "519    @ChristPA Best friend just arrived in NYC from...\n",
       "Name: tweet, dtype: object"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tweets with most positive sentiment\n",
    "semeval_data[semeval_data.textblob_sentiment == 1].tweet.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "327    @progressBFC @loki_poki5 @MBRoberts4004 talkin...\n",
       "383    Boko Haram are so evil. Even on this sacred da...\n",
       "450    I have the worst urge to watch Jurassic World ...\n",
       "592    Putting in my 2 weeks at IHOP tomorrow but pro...\n",
       "732    @jamesjrobertson U2, at this point, may be the...\n",
       "Name: tweet, dtype: object"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reviews with most negative sentiment\n",
    "semeval_data[semeval_data.textblob_sentiment == -1].tweet.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# widen the column display\n",
    "pd.set_option('max_colwidth', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>tweet</th>\n",
       "      <th>tweet_length</th>\n",
       "      <th>textblob_sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5849</th>\n",
       "      <td>261567918196789248</td>\n",
       "      <td>positive</td>\n",
       "      <td>As Port Charles struggles to come to grips with Jason's disappearance...a shocking return!  Tomorrow's episode is can't miss!  Watch #GH!</td>\n",
       "      <td>137</td>\n",
       "      <td>-1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6503</th>\n",
       "      <td>264151907046662144</td>\n",
       "      <td>positive</td>\n",
       "      <td>I wanna see STUDIO FOOTAGE SO BAD. We are about to hear some unreleased STUFF! Likeeeeeeeeee I can't wait until Sunday.</td>\n",
       "      <td>119</td>\n",
       "      <td>-0.875</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      id sentiment  \\\n",
       "5849  261567918196789248  positive   \n",
       "6503  264151907046662144  positive   \n",
       "\n",
       "                                                                                                                                          tweet  \\\n",
       "5849  As Port Charles struggles to come to grips with Jason's disappearance...a shocking return!  Tomorrow's episode is can't miss!  Watch #GH!   \n",
       "6503                    I wanna see STUDIO FOOTAGE SO BAD. We are about to hear some unreleased STUFF! Likeeeeeeeeee I can't wait until Sunday.   \n",
       "\n",
       "      tweet_length  textblob_sentiment  \n",
       "5849           137              -1.000  \n",
       "6503           119              -0.875  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# negative textblob-computed sentiment in a positively labeled tweet\n",
    "semeval_data[(semeval_data.sentiment == \"positive\") & (semeval_data.textblob_sentiment < -0.8)].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>tweet</th>\n",
       "      <th>tweet_length</th>\n",
       "      <th>textblob_sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>625520735151243264</td>\n",
       "      <td>negative</td>\n",
       "      <td>Persons who are favoring Yakub Menen, may be HAPPY on Gurdaspur Attack. Those persons should also be hanged till death along with Yakub.</td>\n",
       "      <td>136</td>\n",
       "      <td>0.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>430770577838968832</td>\n",
       "      <td>negative</td>\n",
       "      <td>it's supposed to snow from midnight tonight until 6pm tomorrow? oh well that's friggin awesome #sarcasm</td>\n",
       "      <td>103</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>637035389920800768</td>\n",
       "      <td>negative</td>\n",
       "      <td>@KylieJenner take a pic with @britneyspears on Sunday PLZZZZZZZZZ!!!!!! The gays will die and go to gay glitter heaven!!!!</td>\n",
       "      <td>122</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216</th>\n",
       "      <td>679527186475429889</td>\n",
       "      <td>negative</td>\n",
       "      <td>Now that Iran ripped us off by the legendary Watch this Friday.</td>\n",
       "      <td>63</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399</th>\n",
       "      <td>624994251189850112</td>\n",
       "      <td>negative</td>\n",
       "      <td>Yup. Great quote @StefanMolyneux : \"He who is w/o sin cast the 1st stone.\" Racist Hulk Hogan Rant...WWE, Gawker &amp;amp; U! https://t.co/pN7SNAl2Sf</td>\n",
       "      <td>144</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     id sentiment  \\\n",
       "64   625520735151243264  negative   \n",
       "72   430770577838968832  negative   \n",
       "122  637035389920800768  negative   \n",
       "216  679527186475429889  negative   \n",
       "399  624994251189850112  negative   \n",
       "\n",
       "                                                                                                                                                tweet  \\\n",
       "64           Persons who are favoring Yakub Menen, may be HAPPY on Gurdaspur Attack. Those persons should also be hanged till death along with Yakub.   \n",
       "72                                            it's supposed to snow from midnight tonight until 6pm tomorrow? oh well that's friggin awesome #sarcasm   \n",
       "122                        @KylieJenner take a pic with @britneyspears on Sunday PLZZZZZZZZZ!!!!!! The gays will die and go to gay glitter heaven!!!!   \n",
       "216                                                                                   Now that Iran ripped us off by the legendary Watch this Friday.   \n",
       "399  Yup. Great quote @StefanMolyneux : \"He who is w/o sin cast the 1st stone.\" Racist Hulk Hogan Rant...WWE, Gawker &amp; U! https://t.co/pN7SNAl2Sf   \n",
       "\n",
       "     tweet_length  textblob_sentiment  \n",
       "64            136                 0.8  \n",
       "72            103                 1.0  \n",
       "122           122                 1.0  \n",
       "216            63                 1.0  \n",
       "399           144                 1.0  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# positive textblob-computed sentiment in a negatively labeled tweet\n",
    "semeval_data[(semeval_data.sentiment == \"negative\") & (semeval_data.textblob_sentiment > 0.7)].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding Extra Features to a Document-Term Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols = ['tweet', 'textblob_sentiment','tweet_length']\n",
    "X = semeval_data[feature_cols]\n",
    "y = semeval_data.target\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "PicklingError",
     "evalue": "Can't pickle <type 'instancemethod'>: it's not found as __builtin__.instancemethod",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPicklingError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-90213e7aa471>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m                         \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstrat_cv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m                         \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m                         verbose=1))\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"text only accuracy: {:0.3f}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtweet_only_acc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python2.7/site-packages/sklearn/model_selection/_validation.pyc\u001b[0m in \u001b[0;36mcross_val_score\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch)\u001b[0m\n\u001b[1;32m    340\u001b[0m                                 \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m                                 \u001b[0mfit_params\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 342\u001b[0;31m                                 pre_dispatch=pre_dispatch)\n\u001b[0m\u001b[1;32m    343\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mcv_results\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'test_score'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python2.7/site-packages/sklearn/model_selection/_validation.pyc\u001b[0m in \u001b[0;36mcross_validate\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score)\u001b[0m\n\u001b[1;32m    204\u001b[0m             \u001b[0mfit_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_train_score\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_train_score\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m             return_times=True)\n\u001b[0;32m--> 206\u001b[0;31m         for train, test in cv.split(X, y, groups))\n\u001b[0m\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreturn_train_score\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    787\u001b[0m                 \u001b[0;31m# consumption.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 789\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    790\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.pyc\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    697\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    698\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'supports_timeout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 699\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    700\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python2.7/multiprocessing/pool.pyc\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    570\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    571\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 572\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_set\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPicklingError\u001b[0m: Can't pickle <type 'instancemethod'>: it's not found as __builtin__.instancemethod"
     ]
    }
   ],
   "source": [
    "# use CountVectorizer with text column only\n",
    "pipeline = Pipeline([(\"countVect\",\n",
    "                      CountVectorizer(stop_words=\"english\", \n",
    "                                      tokenizer = tokenizer_for_tweets.tokenize)),\n",
    "                     (\"lr\",LogisticRegression())])\n",
    "\n",
    "strat_cv = StratifiedKFold(n_splits=10)\n",
    "tweet_only_acc = np.mean(cross_val_score(pipeline,\n",
    "                        X.tweet,\n",
    "                        y,\n",
    "                        scoring=\"accuracy\",\n",
    "                        cv=strat_cv,\n",
    "                        n_jobs=-1,\n",
    "                        verbose=1))\n",
    "print(\"text only accuracy: {:0.3f}\".format(tweet_only_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original matrix:  (7105, 19322)\n"
     ]
    }
   ],
   "source": [
    "print(\"Original matrix: \",CountVectorizer(stop_words=\"english\").fit_transform(X.tweet).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/utils/validation.py:585: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/usr/local/lib/python3.7/site-packages/sklearn/utils/validation.py:585: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/usr/local/lib/python3.7/site-packages/pandas/core/generic.py:4405: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self[name] = value\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "#scale the length feature \n",
    "X.tweet_length = StandardScaler().fit_transform(X.tweet_length.values.reshape((-1,1)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7105, 2)"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cast other feature columns to float and convert to a sparse matrix\n",
    "addl_features = sp.sparse.csr_matrix(X.iloc[:, 1:].astype(float))\n",
    "addl_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix with extra features:  (7105, 20474)\n"
     ]
    }
   ],
   "source": [
    "# combine sparse matrices\n",
    "X_with_addl = sp.sparse.hstack((CountVectorizer(stop_words=\"english\",\n",
    "                                                token_pattern=tokens_re).fit_transform(X.tweet),\n",
    "                                addl_features))\n",
    "print(\"Matrix with extra features: \",X_with_addl.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text and extra features acc: 0.815\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   6 out of  10 | elapsed:    0.4s remaining:    0.2s\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:    0.4s finished\n"
     ]
    }
   ],
   "source": [
    "# use logistic regression with all features\n",
    "text_and_other_features_acc = np.mean(cross_val_score(LogisticRegression(),\n",
    "                                                      X_with_addl,\n",
    "                                                      y,\n",
    "                                                      scoring=\"accuracy\",\n",
    "                                                      cv=strat_cv,\n",
    "                                                      n_jobs=-1,\n",
    "                                                      verbose=1))\n",
    "print(\"text and extra features acc: {:0.3f}\".format(text_and_other_features_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like adding the textblob polarity and tweet length helped a bit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Fun TextBlob Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextBlob(\"15 minutes late\")"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# spelling correction\n",
    "TextBlob('15 minuets laate').correct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('blood', 0.6748768472906403),\n",
       " ('broad', 0.11330049261083744),\n",
       " ('loud', 0.07881773399014778),\n",
       " ('board', 0.04187192118226601),\n",
       " ('cloud', 0.03694581280788178),\n",
       " ('aloud', 0.034482758620689655),\n",
       " ('load', 0.009852216748768473),\n",
       " ('bland', 0.007389162561576354),\n",
       " ('bloated', 0.0024630541871921183)]"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from textblob import Word\n",
    "# spellcheck\n",
    "Word('bloaud').spellcheck()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a farewell remark']"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# definitions - must pass in part of speech you want definitions for\n",
    "Word('goodbye').define()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ru'"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# language identification\n",
    "TextBlob('Ð·Ð´Ñ€Ð°ÑÑ‚Ð²ÑƒÐ¹Ñ‚Ðµ').detect_language()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
