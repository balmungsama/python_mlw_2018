{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An Introduction to Natural Language Processing (NLP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is NLP?\n",
    "\n",
    "Natural language processing (NLP) is the set of techniques developed to automatically process/analyze/understand/ generate natural human languages.\n",
    "\n",
    "### Why is this useful?\n",
    "\n",
    "Most of the knowledge that has accrued over the course of human history is stored as unstructured text and we need some way to make sense out of all of it.\n",
    "\n",
    "**Simply put, NLP enables the automatic, quantitative analysis of this unstructured text.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common NLP subproblems\n",
    "\n",
    "- **Speech recognition and generation**: [Apple siri](https://www.apple.com/ios/siri/)\n",
    "    - Speech to text\n",
    "    - Text to speech\n",
    "- **Question answering**: [IBM Watson](https://www.ibm.com/watson/)\n",
    "    - Match query with knowledge base\n",
    "    - Reasoning about intent of question\n",
    "- **Machine translation**: [Google Translate](https://translate.google.com/)\n",
    "    - One language to another to another\n",
    "- **Information retrieval**: [Google](https://www.google.com/)\n",
    "    - Finding relevant results\n",
    "    - Finding similar results\n",
    "- **Information extraction**: [Gmail](https://www.google.com/gmail/)\n",
    "    - Structured information from unstructured documents\n",
    "- **Assistive technologies**: Google autocompletion\n",
    "    - Predictive text input\n",
    "    - Text simplification\n",
    "- **Natural Language Generation**: [Narrative Science](https://narrativescience.com/)\n",
    "    - Generating text from data\n",
    "- **Automatic summarization**: [Google News](https://news.google.com/news/?ned=us&gl=US&hl=en)\n",
    "    - Extractive summarization\n",
    "    - Abstractive summarization\n",
    "- [**Sentiment analysis**](https://en.wikipedia.org/wiki/Sentiment_analysis):\n",
    "    - Attitude of speaker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are some of the lower level components?\n",
    "\n",
    "- **Tokenization**: breaking text into tokens (words, sentences, n-grams)\n",
    "- **Stopword removal**: \n",
    "    - repetitive & redundant gap-filling utterances (e.g., \"like\") \n",
    "    - bridge words (e.g., a/an/the)\n",
    "- **Stemming and lemmatization**: root word\n",
    "- **TF-IDF**: word importance\n",
    "- **Part-of-speech tagging**: noun/verb/adjective\n",
    "- **Named entity recognition**: person/organization/location\n",
    "- **Spelling correction**: \"New Yrok City\"\n",
    "- **Word sense disambiguation**: \"buy a mouse\"\n",
    "- **Segmentation**: \"New York City subway\"\n",
    "- **Language detection**: \"translate this page\"\n",
    "- **Machine learning**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why is NLP hard?\n",
    "\n",
    "- **Ambiguity**:\n",
    "    - Teacher Strikes Idle Kids\n",
    "    - Red Tape Holds Up New Bridges\n",
    "    - Hospitals are Sued by 7 Foot Doctors\n",
    "    - Juvenile Court to Try Shooting Defendant\n",
    "    - Local High School Dropouts Cut in Half\n",
    "- **Non-standard English**: tweets/text messages\n",
    "- **Idioms**: \"throw in the towel\"\n",
    "- **Newly coined words**: \"retweet\"\n",
    "- **Tricky entity names**: \"Where is A Bug's Life playing?\"\n",
    "- **World knowledge**: \"Mary and Sue are sisters\", \"Mary and Sue are mothers\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How does NLP work?\n",
    "\n",
    "- Build probabilistic model using data about a language\n",
    "- Requires an understanding of the language\n",
    "- Requires an understanding of the world (or a particular domain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading in a subset of SemEval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dependencies #\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "\n",
    "# for unbalanced data \n",
    "from sklearn.model_selection import train_test_split, StratifiedShuffleSplit\n",
    "\n",
    "# vectorizer (this one ignores the syntax -i.e., order and focuses on frequency)\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "# \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the reviews into a DataFrame\n",
    "semeval_data = pd.read_csv(\"../data/semeval_sampled_cleaned_data.csv\",sep=\"\\t\",names=[\"id\",\"sentiment\",\"tweet\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7105, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>633616008910082048</td>\n",
       "      <td>negative</td>\n",
       "      <td>Donald Trump and Scott Walker would Negros bac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>639974060466663424</td>\n",
       "      <td>negative</td>\n",
       "      <td>@YidVids2 probably not cause he played with th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>664049298624114688</td>\n",
       "      <td>negative</td>\n",
       "      <td>Woaw just because briana is \"having\" louis' ba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>665627471899975680</td>\n",
       "      <td>negative</td>\n",
       "      <td>I wrote this about the 'SAS response' after th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>522951269271339008</td>\n",
       "      <td>negative</td>\n",
       "      <td>@MasterDebator_ @NFLosophy 2nd best in luck dr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>109016744177319937</td>\n",
       "      <td>negative</td>\n",
       "      <td>Boehner tells Obama that sorry, the House is a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>641475286228271108</td>\n",
       "      <td>negative</td>\n",
       "      <td>I think Google may be worried that if they all...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>636734909646815233</td>\n",
       "      <td>negative</td>\n",
       "      <td>This goes right up there with Rolling Stone pu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>636155339067424768</td>\n",
       "      <td>negative</td>\n",
       "      <td>If Carly Fiorina ran the US the way she ran HP...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>111733236627025920</td>\n",
       "      <td>negative</td>\n",
       "      <td>@HartsPub ru showing the All Blacks v Tonga ga...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   id sentiment  \\\n",
       "0  633616008910082048  negative   \n",
       "1  639974060466663424  negative   \n",
       "2  664049298624114688  negative   \n",
       "3  665627471899975680  negative   \n",
       "4  522951269271339008  negative   \n",
       "5  109016744177319937  negative   \n",
       "6  641475286228271108  negative   \n",
       "7  636734909646815233  negative   \n",
       "8  636155339067424768  negative   \n",
       "9  111733236627025920  negative   \n",
       "\n",
       "                                               tweet  \n",
       "0  Donald Trump and Scott Walker would Negros bac...  \n",
       "1  @YidVids2 probably not cause he played with th...  \n",
       "2  Woaw just because briana is \"having\" louis' ba...  \n",
       "3  I wrote this about the 'SAS response' after th...  \n",
       "4  @MasterDebator_ @NFLosophy 2nd best in luck dr...  \n",
       "5  Boehner tells Obama that sorry, the House is a...  \n",
       "6  I think Google may be worried that if they all...  \n",
       "7  This goes right up there with Rolling Stone pu...  \n",
       "8  If Carly Fiorina ran the US the way she ran HP...  \n",
       "9  @HartsPub ru showing the All Blacks v Tonga ga...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(semeval_data.shape)\n",
    "semeval_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "semeval_data.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "positive    0.511612\n",
       "negative    0.488388\n",
       "Name: sentiment, dtype: float64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# frequency of positive vs. negative tweets\n",
    "semeval_data.sentiment.value_counts()/semeval_data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I wish I was in Bolton tonight :('"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sample & read a random tweet\n",
    "semeval_data.tweet[20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Terminology**\n",
    "- **corpus:** collection of documents \n",
    "    - document: individual row in the data -i.e., a tweet\n",
    "- **corpora:** plural form of corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting tweets to utf (best encoding method up to date)\n",
    "semeval_data.tweet = semeval_data.tweet.map(lambda x: x.encode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting sentiment as 1's and 0's\n",
    "semeval_data[\"target\"] = (semeval_data.sentiment==\"positive\").astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the new DataFrame into training and testing sets, keeping relative frequencies of targets unchanged\n",
    "splitter = StratifiedShuffleSplit(n_splits=1,\n",
    "                                  test_size=0.3)\n",
    "train_indices,test_indices=list(splitter.split(semeval_data.tweet,semeval_data.target))[0]\n",
    "X_train,y_train = semeval_data.tweet.iloc[train_indices],semeval_data.target.iloc[train_indices]\n",
    "X_test,y_test = semeval_data.tweet.iloc[test_indices],semeval_data.target.iloc[test_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure training dataset matches the frequency of +ve and -ve sentiment frequency\n",
    "y_train.value_counts()/y_train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.value_counts()/y_test.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **What:** Separate text into units such as sentences or words\n",
    "- **Why:** Gives structure to previously unstructured text\n",
    "- **Notes:** Relatively easy with English language text, not easy with some languages\n",
    "- **Tweets:** Traditionally in NLP, you throw out any non alphabetical characters/symbols and numbers. With tweets, certain additional symbols convey meaning (hashtags, emoticons, etc.), so we will need to parse tweets a bit differently than we would a normal piece of text.\n",
    "\n",
    "Here we import a special tweet-specific tokenizer that can handle what is described above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    " \n",
    "emoticons_str = r\"\"\"\n",
    "    (?:\n",
    "        [:=;] # Eyes\n",
    "        [oO\\-]? # Nose (optional)\n",
    "        [D\\)\\]\\(\\]/\\\\OpP] # Mouth\n",
    "    )\"\"\"\n",
    " \n",
    "regex_str = [\n",
    "    emoticons_str,\n",
    "    r'<[^>]+>', # HTML tags\n",
    "    r'(?:@[\\w_]+)', # @-mentions\n",
    "    r\"(?:\\#+[\\w_]+[\\w\\'_\\-]*[\\w_]+)\", # hash-tags\n",
    "    r'http[s]?://(?:[a-z]|[0-9]|[$-_@.&amp;+]|[!*\\(\\),]|(?:%[0-9a-f][0-9a-f]))+', # URLs\n",
    " \n",
    "    r'(?:(?:\\d+,?)+(?:\\.?\\d+)?)', # numbers\n",
    "    r\"(?:[a-z][a-z'\\-_]+[a-z])\", # words with - and '\n",
    "    r'(?:[\\w_]+)', # other words\n",
    "    r'(?:\\S)' # anything else\n",
    "]\n",
    "    \n",
    "tokens_re = re.compile(r'('+'|'.join(regex_str)+')', re.VERBOSE | re.IGNORECASE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer \n",
    "from nltk.tokenize import TweetTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_for_tweets = TweetTokenizer(strip_handles = True, # remove @'s\n",
    "                           preserve_case = False, # otherwise upper and lower-cases are treated as different words\n",
    "                           reduce_len = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **token_pattern:** string\n",
    "- Regular expression denoting what constitutes a \"token\". The default regexp selects tokens of 2 or more alphanumeric characters (punctuation is completely ignored and always treated as a token separator).\n",
    "- **tokenizer:** callable/function\n",
    "- Function that converts a string into a list of tokens using some arbitrary logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use CountVectorizer to create document-term matrices from X_train and X_test\n",
    "## dictionary contains unique tokens extracted from documents\n",
    "vect = CountVectorizer(tokenizer = tokenizer_for_tweets.tokenize)\n",
    "\n",
    "# training data properties must be applied to tested on testing data (e.g., tf-idf)\n",
    "train_dtm = vect.fit_transform(X_train) # counting the tokens\n",
    "test_dtm = vect.transform(X_test) # transforming is also doing the same thing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rows are documents, columns are terms (aka \"tokens\" or \"features\")\n",
    "print(\"# of tokens in training data:\", train_dtm.shape[1])\n",
    "print(\"# of tokens in testing data:\", test_dtm.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# last 50 features\n",
    "print(vect.get_feature_names()[-50:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compressed sparse format for a large dataset\n",
    "train_dtm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show vectorizer options\n",
    "vect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the [CountVectorizer documentation](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) to get a better understanding of how it works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **lowercase:** `boolean`, `True` by default\n",
    "- Convert all characters to lowercase before tokenizing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# don't convert to lowercase\n",
    "vect = CountVectorizer(lowercase = False,\n",
    "                       tokenizer = tokenizer_for_tweets.tokenize)\n",
    "\n",
    "train_dtm = vect.fit_transform(X_train)\n",
    "train_dtm.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the impact of not lowercasing the text?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# allow tokens of one character\n",
    "vect = CountVectorizer(token_pattern = r'(?u)\\b\\w+\\b')\n",
    "\n",
    "train_dtm = vect.fit_transform(X_train)\n",
    "\n",
    "# number of tokens \n",
    "print(\"# of tokens for this tokenizer:\", train_dtm.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **ngram_range:** tuple (min_n, max_n)\n",
    "- The lower and upper boundary of the range of n-values for different n-grams to be extracted. All values of n such that min_n <= n <= max_n will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# include 1-grams and 2-grams\n",
    "vect = CountVectorizer(ngram_range = (1, 2), # min = 1, max = 2\n",
    "                       tokenizer = tokenizer_for_tweets.tokenize)\n",
    "train_dtm = vect.fit_transform(X_train)\n",
    "\n",
    "print(\"# of tokens for this tokenizer:\", train_dtm.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# last 50 features\n",
    "print(vect.get_feature_names()[-50:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now, build a LR model predicting sentiment:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use default options for CountVectorizer\n",
    "vect = CountVectorizer(tokenizer = tokenizer.tokenize)\n",
    "\n",
    "# create document-term matrices\n",
    "train_dtm = vect.fit_transform(X_train)\n",
    "test_dtm = vect.transform(X_test)\n",
    "\n",
    "# use Logistic Regression to predict the star rating\n",
    "lr = LogisticRegression()\n",
    "lr.fit(train_dtm, y_train)\n",
    "y_pred_class = lr.predict(test_dtm)\n",
    "\n",
    "# calculate accuracy\n",
    "print(metrics.accuracy_score(y_test, y_pred_class).round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate null accuracy\n",
    "y_test.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.coef_.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome, so just using raw counts of different words gives us very good performance!\n",
    "\n",
    "Let's examine the most positive and most negative words (their coefficients will tell us whether they are most indicative of a positive or negative review)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zip objects extentiate the two vectors \n",
    "a = [1, 2, 3, 4]\n",
    "b = [\"a\", \"b\", \"c\", \"d\"]\n",
    "\n",
    "list(zip(a, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_coeffs = pd.DataFrame(list(zip(vect.get_feature_names(), \n",
    "                                       lr.coef_[0])),\n",
    "                              columns=[\"word\",\"coeff\"]) # naming the two columns\n",
    "feature_coeffs = feature_coeffs.sort_values(by=\"coeff\",ascending=False).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Words most indicative of positive review:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# most positive words\n",
    "feature_coeffs.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Words most indicative of negative review:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# most negative words\n",
    "feature_coeffs.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_coeffs[\"abs_coeff\"] = feature_coeffs.coeff.abs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most predictive words, regardless of polarity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_coeffs.sort_values(by = \"abs_coeff\",\n",
    "                           inplace = True,\n",
    "                           ascending = False)\n",
    "feature_coeffs.head(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Useless words (dont tell you anything about the polarity of the review):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_coeffs.tail(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipelines To Make CV/Transformations Easier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_pipeline = Pipeline([(\"countVect\", # string that explains what it does\n",
    "                            CountVectorizer(tokenizer = tokenizer_for_tweets.tokenize)), # step 1\n",
    "                           (\"lr\",LogisticRegression())]) # step 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# passing the raw data through the defined pipeline\n",
    "first_pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy based on actual testing data and prediction \n",
    "## \".predict\" indicates it's an estimator\n",
    "print(metrics.accuracy_score(y_test, first_pipeline.predict(X_test)).round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strat_cv = StratifiedKFold(n_splits=10) # balance splits by frequency of each target class\n",
    "# cv = 10 #10-fold cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# average accuracy over 10 strafied cross-validations \n",
    "np.mean(cross_val_score(first_pipeline,\n",
    "                semeval_data.tweet,\n",
    "                semeval_data.target,\n",
    "                scoring = \"accuracy\",\n",
    "                cv = strat_cv,\n",
    "                n_jobs = -1, # parallel processing\n",
    "                verbose = 1)).round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fitting data in the entire dataset\n",
    "first_pipeline.fit(semeval_data.tweet, semeval_data.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to access the innards\n",
    "first_pipeline.steps[0][1].get_feature_names() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = first_pipeline.steps[0][1].get_feature_names()\n",
    "feature_coeffs = first_pipeline.steps[1][1].coef_\n",
    "\n",
    "feature_coeffs = pd.DataFrame(list(zip(feature_names,feature_coeffs.reshape((-1)))), \n",
    "                              columns = [\"word\",\"coeff\"])\n",
    "feature_coeffs = feature_coeffs.sort_values(by = \"coeff\",\n",
    "                                            ascending = False).reset_index(drop = True)\n",
    " \n",
    "feature_coeffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_pipeline.steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets create a function that accepts a vectorizer and returns a table with the coefficients and accuracy of cv-ed model\n",
    "def tokenize_test(vect,clf):\n",
    "    pipe = Pipeline([(\"vect\",vect),(\"lr\",clf)])\n",
    "    pipe.fit(semeval_data.tweet,semeval_data.target)\n",
    "    num_features = len(pipe.steps[0][1].get_feature_names())\n",
    "    print('Num Features: ', num_features)\n",
    "\n",
    "    zipped_coeffs = list(zip(pipe.steps[0][1].get_feature_names(),\n",
    "                             pipe.steps[1][1].coef_[0]))\n",
    "    feature_coeffs = pd.DataFrame(zipped_coeffs,columns=[\"word\",\"coeff\"]).sort_values(by=\"coeff\",ascending=False)\n",
    "    feature_coeffs.reset_index(drop=True,inplace=True)\n",
    "\n",
    "    strat_cv = StratifiedKFold(n_splits=10)\n",
    "    acc = np.mean(cross_val_score(pipe,\n",
    "                                  semeval_data.tweet,\n",
    "                                  semeval_data.target,\n",
    "                                  scoring=\"accuracy\",\n",
    "                                  cv=strat_cv,\n",
    "                                  n_jobs=-1,\n",
    "                                  verbose=1))\n",
    "    print(\"Accuracy:\", acc)\n",
    "    return (feature_coeffs, acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# include 1-grams and 2-grams\n",
    "vect = CountVectorizer(tokenizer=tokenizer_for_tweets.tokenize,\n",
    "                       ngram_range=(1,2))\n",
    "feature_coeffs,acc = tokenize_test(vect,LogisticRegression())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_coeffs.head(10) # most positive words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_coeffs.sort_values(\"coeff\").head(10) # most negative words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_coeffs[\"abs_coeffs\"] = feature_coeffs.coeff.abs() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_coeffs.sort_values(\"abs_coeffs\",ascending=False).head(10) # absolute values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stopword Removal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **What:** Remove common words that will likely appear in any text\n",
    "- **Why:** They don't tell you much about your text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show vectorizer options\n",
    "vect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **stop_words:** `string` {`'english'`}, `list`, or `None` (default)\n",
    "- If `'english'`, a built-in stop word list for English is used.\n",
    "- If a `list`, that list is assumed to contain stop words, all of which will be removed from the resulting tokens.\n",
    "- If `None`, no stop words will be used. \n",
    "- `max_df` can be set to a value in the range [0.7, 1.0) to automatically detect and filter stop words based on within-corpus document frequency of terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove English stop words\n",
    "vect = CountVectorizer(stop_words = 'english',\n",
    "                       tokenizer = tokenizer_for_tweets.tokenize)\n",
    "\n",
    "feature_coeffs,acc = tokenize_test(vect,\n",
    "                                   LogisticRegression())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_coeffs.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_coeffs.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_coeffs[\"abs_coeffs\"] = feature_coeffs.coeff.abs()\n",
    "feature_coeffs.sort_values(\"abs_coeffs\").head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set of stop words\n",
    "print(vect.get_stop_words())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other CountVectorizer Options"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **max_features:** int or None, default=None\n",
    "- If not None, build a vocabulary that only consider the top max_features ordered by term frequency across the corpus.\n",
    "\n",
    "- **min_df:** float in range [0.0, 1.0] or int, default=1\n",
    "- When building the vocabulary ignore terms that have a document frequency strictly lower than the given threshold. This value is also called cut-off in the literature. If float, the parameter represents a proportion of documents, integer absolute counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove English stop words and only keep words appearing in 0.5% of documents\n",
    "#and never appear in more than 70% of documents\n",
    "vect = CountVectorizer(stop_words ='english', \n",
    "                       min_df = 0.005, \n",
    "                       max_df = 0.7,\n",
    "                       tokenizer = tokenizer_for_tweets.tokenize)\n",
    "\n",
    "features,acc = tokenize_test(vect,\n",
    "                             LogisticRegression())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features.sort_values(\"coeff\").head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# include 1-grams and 2-grams, and only include terms that appear at least 20 times\n",
    "vect = CountVectorizer(ngram_range=(1, 2), \n",
    "                       min_df=20,\n",
    "                       tokenizer = tokenizer_for_tweets.tokenize)\n",
    "\n",
    "features,acc = tokenize_test(vect,LogisticRegression())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features.sort_values(\"coeff\").head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features[\"abs_coeffs\"] = features.coeff.abs()\n",
    "features.sort_values(\"abs_coeffs\").head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Term Frequency - Inverse Document Frequency (TF-IDF) Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **What:** Computes \"relative frequency\" that a word appears in a document compared to its frequency across all documents\n",
    "- **Why:** More useful than \"term frequency\" for identifying \"important\" words in each document \n",
    "    - high frequency in that document, low frequency in other documents\n",
    "- **Notes:** Used for search engine scoring, text summarization, document clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example documents\n",
    "train_simple = ['call you tonight',\n",
    "                'Call me a cab',\n",
    "                'please call me... PLEASE!']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CountVectorizer\n",
    "vect = CountVectorizer()\n",
    "\n",
    "pd.DataFrame(vect.fit_transform(train_simple).toarray(), \n",
    "             columns = vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TfidfVectorizer\n",
    "vect = TfidfVectorizer()\n",
    "pd.DataFrame(vect.fit_transform(train_simple).toarray(), \n",
    "             columns = vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vect = TfidfVectorizer(tokenizer = tokenizer_for_tweets.tokenize)\n",
    "tfidf_coeffs,acc = tokenize_test(vect,LogisticRegression())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for max_features in (1000,10000):\n",
    "    vect = TfidfVectorizer(max_features = max_features,\n",
    "                           tokenizer = tokenizer_for_tweets.tokenize)\n",
    "    tokenize_test(vect,LogisticRegression())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ngram_range in (2,3):\n",
    "    vect = TfidfVectorizer(max_features = 10000,\n",
    "                           ngram_range = (1,ngram_range),\n",
    "                           tokenizer = tokenizer_for_tweets.tokenize)\n",
    "    tokenize_test(vect,LogisticRegression())    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tfidf_coeffs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tfidf_coeffs.sort_values(\"coeff\").head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_coeffs[\"abs_coeffs\"] = tfidf_coeffs.coeff.abs()\n",
    "tfidf_coeffs.sort_values(\"abs_coeffs\").head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Introduction to TextBlob](http://textblob.readthedocs.io/en/dev/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the first tweet\n",
    "print(semeval_data.tweet.values[20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet = textblob.TextBlob(semeval_data.tweet.values[20].decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the command below fails, run `conda install -c conda-forge textblob ` from a terminal window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download(\"punkt\")\n",
    "# list the words\n",
    "tweet.words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list the sentences\n",
    "tweet.sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some string methods are available\n",
    "tweet.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming and Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Stemming:**\n",
    "\n",
    "- **What:** Reduce a word to its base/stem/root form\n",
    "- **Why:** Often makes sense to treat related words the same way\n",
    "- **Notes:**\n",
    "    - Uses a \"simple\" and fast rule-based approach\n",
    "    - Stemmed words are usually not shown to users (used for analysis/indexing)\n",
    "    - Some search engines treat words with the same stem as synonyms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the import below fails, run `conda install -c conda-forge nltk` in a terminal window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize stemmer\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "stemmer = SnowballStemmer('english')\n",
    "\n",
    "# stem each word\n",
    "print([stemmer.stem(word) for word in tweet.words])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lemmatization**\n",
    "\n",
    "- **What:** Derive the canonical form ('lemma') of a word\n",
    "- **Why:** Can be better than stemming\n",
    "- **Notes:** Uses a dictionary-based approach (slower than stemming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download(\"wordnet\")\n",
    "# assume every word is a noun\n",
    "print([word.lemmatize() for word in tweet.words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assume every word is a verb\n",
    "print([word.lemmatize(pos='v') for word in tweet.words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function that accepts text and returns a list of lemmas\n",
    "def split_into_lemmas(text):\n",
    "    text = text.lower()\n",
    "    words = textblob.TextBlob(text.decode(\"utf-8\")).words\n",
    "    return [word.lemmatize() for word in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use split_into_lemmas as the feature extraction function\n",
    "vect = CountVectorizer(analyzer=split_into_lemmas)\n",
    "features,acc = tokenize_test(vect,LogisticRegression())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features.sort_values(\"coeff\").head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis in TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# polarity ranges from -1 (most negative) to 1 (most positive)\n",
    "tweet.sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "semeval_data['tweet_length'] = semeval_data.tweet.apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function that accepts text and returns the polarity\n",
    "def detect_sentiment(text):\n",
    "    return textblob.TextBlob(text.decode(\"utf-8\")).sentiment.polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new DataFrame column for sentiment\n",
    "semeval_data['textblob_sentiment'] = semeval_data.tweet.apply(detect_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# boxplot of sentiment grouped by stars\n",
    "semeval_data.boxplot(column='textblob_sentiment', by='sentiment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tweets with most positive sentiment\n",
    "semeval_data[semeval_data.textblob_sentiment == 1].tweet.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reviews with most negative sentiment\n",
    "semeval_data[semeval_data.textblob_sentiment == -1].tweet.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# widen the column display\n",
    "pd.set_option('max_colwidth', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# negative textblob-computed sentiment in a positively labeled tweet\n",
    "semeval_data[(semeval_data.sentiment == \"positive\") & (semeval_data.textblob_sentiment < -0.8)].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# positive textblob-computed sentiment in a negatively labeled tweet\n",
    "semeval_data[(semeval_data.sentiment == \"negative\") & (semeval_data.textblob_sentiment > 0.7)].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding Extra Features to a Document-Term Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols = ['tweet', 'textblob_sentiment','tweet_length']\n",
    "X = semeval_data[feature_cols]\n",
    "y = semeval_data.target\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use CountVectorizer with text column only\n",
    "pipeline = Pipeline([(\"countVect\",\n",
    "                      CountVectorizer(stop_words=\"english\", \n",
    "                                      tokenizer = tokenizer_for_tweets.tokenize)),\n",
    "                     (\"lr\",LogisticRegression())])\n",
    "\n",
    "strat_cv = StratifiedKFold(n_splits=10)\n",
    "tweet_only_acc = np.mean(cross_val_score(pipeline,\n",
    "                        X.tweet,\n",
    "                        y,\n",
    "                        scoring=\"accuracy\",\n",
    "                        cv=strat_cv,\n",
    "                        n_jobs=-1,\n",
    "                        verbose=1))\n",
    "print(\"text only accuracy: {:0.3f}\".format(tweet_only_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Original matrix: \",CountVectorizer(stop_words=\"english\").fit_transform(X.tweet).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "#scale the length feature \n",
    "X.tweet_length = StandardScaler().fit_transform(X.tweet_length.values.reshape((-1,1)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cast other feature columns to float and convert to a sparse matrix\n",
    "addl_features = sp.sparse.csr_matrix(X.iloc[:, 1:].astype(float))\n",
    "addl_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine sparse matrices\n",
    "X_with_addl = sp.sparse.hstack((CountVectorizer(stop_words=\"english\",\n",
    "                                                token_pattern=tokens_re).fit_transform(X.tweet),\n",
    "                                addl_features))\n",
    "print(\"Matrix with extra features: \",X_with_addl.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use logistic regression with all features\n",
    "text_and_other_features_acc = np.mean(cross_val_score(LogisticRegression(),\n",
    "                                                      X_with_addl,\n",
    "                                                      y,\n",
    "                                                      scoring=\"accuracy\",\n",
    "                                                      cv=strat_cv,\n",
    "                                                      n_jobs=-1,\n",
    "                                                      verbose=1))\n",
    "print(\"text and extra features acc: {:0.3f}\".format(text_and_other_features_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like adding the textblob polarity and tweet length helped a bit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Fun TextBlob Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spelling correction\n",
    "TextBlob('15 minuets laate').correct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import Word\n",
    "# spellcheck\n",
    "Word('bloaud').spellcheck()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# definitions - must pass in part of speech you want definitions for\n",
    "Word('goodbye').define()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# language identification\n",
    "TextBlob('здраствуйте').detect_language()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
