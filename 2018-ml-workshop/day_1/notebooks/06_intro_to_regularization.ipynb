{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Agenda:\n",
    "  * **Regularization: Lasso Regression (L1 regularization for Logistic Regression)**\n",
    "  * **Regularization: Ridge Regression (L2 regularization for Logistic Regression)**\n",
    "  * **Regularization: Elastic Net Regression (L1 and L2 combined regularization for Logistic Regression)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to talk about what happens when we have lots of predictor variables (columns/features), and are trying to both generate accurate models, and reduce the number of features we use to predict/explain what is going on in our dataset.\n",
    "\n",
    "We may need to do this for a variety of reasons.\n",
    "\n",
    "In many instances, the number of columns you have in your data can be much larger than the number of samples (examples/rows) you have, this is especially the case with neuroscientific data where the number of channels/sensors is usually much, much larger than the number of trials you have (think of voxels in an ROI or channels for an EEG or MEG rig). One unfortunate result of this can be that we **overfit** our models (many different combinations of the columns in our data can generate the same training results). We need a systematic way to reduce the number of columns we use in our models to just those columns that are the most important for our prediction.\n",
    "\n",
    "In general, not all of the columns in our dataset may be critical for predicting a given outcome variable because:\n",
    "\n",
    "1. Several columns may be highly correlated with each other (provide redundant information) and with the outcome variable.\n",
    "2. Columns may not be correlated at all with the outcome attribute (have no \"predictive ability\" in terms of what you're trying to predict).\n",
    "\n",
    "One common strategy for dealing with both of these cases is called and **Regularization.**\n",
    "\n",
    "By the end of this lesson you will be able to:\n",
    "\n",
    "  * use Lasso/L1 regularization for both feature selection and general regression/classification problems\n",
    "  * use Ridge/L2 regularization for regression and classification problems\n",
    "  * explain the difference between L1/L2 regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data handling, model creation/evaluation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import Lasso, Ridge, LogisticRegression, ElasticNet\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "import scipy.stats as stats\n",
    "\n",
    "# visualization\n",
    "%matplotlib inline\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to use our [chronic kidney disease dataset](http://archive.ics.uci.edu/ml/datasets/Housing) to explore Regularization.\n",
    "\n",
    "Recall the dataset attributes:\n",
    "\n",
    "* **age**: subject age, numeric\n",
    "* **bp**: blood pressure,  numeric\n",
    "* **sg**: specific gravity, numeric but discrete\n",
    "* **al**: albumin, numeric but discrete\n",
    "* **su**: sugar, numeric but discrete\n",
    "* **rbc**: red blood cells, categorical\n",
    "* **pc**: pus cell, categorical \n",
    "* **pcc**: pus cell clumps, categorical\n",
    "* **ba**: bacteria, categorical\n",
    "* **bgr**: blood glucose random, numeric\n",
    "* **bu**: blood urea, numeric\n",
    "* **sc**: serum creatinine, numeric \n",
    "* **sod**: sodium, numeric \n",
    "* **pot**: potassium, numeric \n",
    "* **hemo**: hemoglobin, numeric\n",
    "* **pcv**: packed cell volume, numeric\n",
    "* **wc**: white blood cell count, numeric\n",
    "* **rc**: red blood cell count, numeric\n",
    "* **htn**: hypertension, categorical\n",
    "* **dm**: diabetes mellitus, categorical \n",
    "* **cad**: coronary artery disease, categorical\n",
    "* **appet**: appetite, categorical\n",
    "* **pe**: pedal edema, categorical \n",
    "* **ane**: anemia, categorical\n",
    "* **class**: class, categorical\n",
    "  \n",
    "Let's load the data in and preprocess as we have in the past:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>bp</th>\n",
       "      <th>sg</th>\n",
       "      <th>al</th>\n",
       "      <th>su</th>\n",
       "      <th>rbc</th>\n",
       "      <th>pc</th>\n",
       "      <th>pcc</th>\n",
       "      <th>ba</th>\n",
       "      <th>bgr</th>\n",
       "      <th>...</th>\n",
       "      <th>pcv</th>\n",
       "      <th>wc</th>\n",
       "      <th>rc</th>\n",
       "      <th>htn</th>\n",
       "      <th>dm</th>\n",
       "      <th>cad</th>\n",
       "      <th>appet</th>\n",
       "      <th>pe</th>\n",
       "      <th>ane</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>48.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>1.020</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>normal</td>\n",
       "      <td>notpresent</td>\n",
       "      <td>notpresent</td>\n",
       "      <td>121.0</td>\n",
       "      <td>...</td>\n",
       "      <td>44.0</td>\n",
       "      <td>7800.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>good</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>ckd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>1.020</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>normal</td>\n",
       "      <td>notpresent</td>\n",
       "      <td>notpresent</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>38.0</td>\n",
       "      <td>6000.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>good</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>ckd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>62.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>1.010</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>normal</td>\n",
       "      <td>normal</td>\n",
       "      <td>notpresent</td>\n",
       "      <td>notpresent</td>\n",
       "      <td>423.0</td>\n",
       "      <td>...</td>\n",
       "      <td>31.0</td>\n",
       "      <td>7500.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>poor</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>ckd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>48.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>1.005</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>normal</td>\n",
       "      <td>abnormal</td>\n",
       "      <td>present</td>\n",
       "      <td>notpresent</td>\n",
       "      <td>117.0</td>\n",
       "      <td>...</td>\n",
       "      <td>32.0</td>\n",
       "      <td>6700.0</td>\n",
       "      <td>3.9</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>poor</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>ckd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>51.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>1.010</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>normal</td>\n",
       "      <td>normal</td>\n",
       "      <td>notpresent</td>\n",
       "      <td>notpresent</td>\n",
       "      <td>106.0</td>\n",
       "      <td>...</td>\n",
       "      <td>35.0</td>\n",
       "      <td>7300.0</td>\n",
       "      <td>4.6</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>good</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>ckd</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    age    bp     sg   al   su     rbc        pc         pcc          ba  \\\n",
       "0  48.0  80.0  1.020  1.0  0.0     NaN    normal  notpresent  notpresent   \n",
       "1   7.0  50.0  1.020  4.0  0.0     NaN    normal  notpresent  notpresent   \n",
       "2  62.0  80.0  1.010  2.0  3.0  normal    normal  notpresent  notpresent   \n",
       "3  48.0  70.0  1.005  4.0  0.0  normal  abnormal     present  notpresent   \n",
       "4  51.0  80.0  1.010  2.0  0.0  normal    normal  notpresent  notpresent   \n",
       "\n",
       "     bgr  ...    pcv      wc   rc  htn   dm  cad  appet   pe  ane class  \n",
       "0  121.0  ...   44.0  7800.0  5.2  yes  yes   no   good   no   no   ckd  \n",
       "1    NaN  ...   38.0  6000.0  NaN   no   no   no   good   no   no   ckd  \n",
       "2  423.0  ...   31.0  7500.0  NaN   no  yes   no   poor   no  yes   ckd  \n",
       "3  117.0  ...   32.0  6700.0  3.9  yes   no   no   poor  yes  yes   ckd  \n",
       "4  106.0  ...   35.0  7300.0  4.6   no   no   no   good   no   no   ckd  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kidney_columns = [\"age\",\"bp\",\"sg\",\"al\",\"su\",\"rbc\",\"pc\",\"pcc\",\"ba\",\n",
    "                  \"bgr\",\"bu\",\"sc\",\"sod\",\"pot\",\"hemo\",\"pcv\",\"wc\",\"rc\",\n",
    "                  \"htn\",\"dm\",\"cad\",\"appet\",\"pe\",\"ane\",\"class\"]\n",
    "kidney_data = pd.read_csv(\"../data/chronic_kidney_disease.csv\", names=kidney_columns,na_values=\"?\")\n",
    "kidney_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>bp</th>\n",
       "      <th>sg</th>\n",
       "      <th>al</th>\n",
       "      <th>su</th>\n",
       "      <th>bgr</th>\n",
       "      <th>bu</th>\n",
       "      <th>sc</th>\n",
       "      <th>sod</th>\n",
       "      <th>pot</th>\n",
       "      <th>...</th>\n",
       "      <th>pc</th>\n",
       "      <th>pcc</th>\n",
       "      <th>ba</th>\n",
       "      <th>htn</th>\n",
       "      <th>dm</th>\n",
       "      <th>cad</th>\n",
       "      <th>appet</th>\n",
       "      <th>pe</th>\n",
       "      <th>ane</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>48.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>1.020</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>1.2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>normal</td>\n",
       "      <td>notpresent</td>\n",
       "      <td>notpresent</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>good</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>ckd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>1.020</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>18.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>normal</td>\n",
       "      <td>notpresent</td>\n",
       "      <td>notpresent</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>good</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>ckd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>62.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>1.010</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>423.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>1.8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>normal</td>\n",
       "      <td>notpresent</td>\n",
       "      <td>notpresent</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>poor</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>ckd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>48.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>1.005</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>117.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>3.8</td>\n",
       "      <td>111.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>...</td>\n",
       "      <td>abnormal</td>\n",
       "      <td>present</td>\n",
       "      <td>notpresent</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>poor</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>ckd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>51.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>1.010</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>106.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>normal</td>\n",
       "      <td>notpresent</td>\n",
       "      <td>notpresent</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>good</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>ckd</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    age    bp     sg   al   su    bgr    bu   sc    sod  pot  ...         pc  \\\n",
       "0  48.0  80.0  1.020  1.0  0.0  121.0  36.0  1.2    NaN  NaN  ...     normal   \n",
       "1   7.0  50.0  1.020  4.0  0.0    NaN  18.0  0.8    NaN  NaN  ...     normal   \n",
       "2  62.0  80.0  1.010  2.0  3.0  423.0  53.0  1.8    NaN  NaN  ...     normal   \n",
       "3  48.0  70.0  1.005  4.0  0.0  117.0  56.0  3.8  111.0  2.5  ...   abnormal   \n",
       "4  51.0  80.0  1.010  2.0  0.0  106.0  26.0  1.4    NaN  NaN  ...     normal   \n",
       "\n",
       "          pcc          ba  htn   dm cad appet   pe  ane class  \n",
       "0  notpresent  notpresent  yes  yes  no  good   no   no   ckd  \n",
       "1  notpresent  notpresent   no   no  no  good   no   no   ckd  \n",
       "2  notpresent  notpresent   no  yes  no  poor   no  yes   ckd  \n",
       "3     present  notpresent  yes   no  no  poor  yes  yes   ckd  \n",
       "4  notpresent  notpresent   no   no  no  good   no   no   ckd  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kidney_columns = kidney_columns[:5]+kidney_columns[9:18]+kidney_columns[5:9]+kidney_columns[18:]\n",
    "kidney_data = kidney_data[kidney_columns]\n",
    "kidney_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. for the categorical data\n",
    "#function to return the most frequent value in a pandas Series\n",
    "def get_most_frequent_value(my_column):\n",
    "    return my_column.value_counts().index[0]\n",
    "\n",
    "most_frequent_values_per_column = kidney_data[kidney_columns[14:-1]].apply(get_most_frequent_value,axis=0)\n",
    "categorical_most_frequent = kidney_data[kidney_columns[14:-1]].fillna(most_frequent_values_per_column,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. this is where we will replace categoricals with values\n",
    "no_map = {\"no\":0,\"yes\":1}\n",
    "norm_map = {\"normal\":0,\"abnormal\":1}\n",
    "pres_map = {\"notpresent\":0,\"present\":1}\n",
    "good_map = {\"poor\":0,\"good\":1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rbc</th>\n",
       "      <th>pc</th>\n",
       "      <th>pcc</th>\n",
       "      <th>ba</th>\n",
       "      <th>htn</th>\n",
       "      <th>dm</th>\n",
       "      <th>cad</th>\n",
       "      <th>appet</th>\n",
       "      <th>pe</th>\n",
       "      <th>ane</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   rbc  pc  pcc  ba  htn  dm  cad  appet  pe  ane\n",
       "0    0   0    0   0    1   1    0      1   0    0\n",
       "1    0   0    0   0    0   0    0      1   0    0\n",
       "2    0   0    0   0    0   1    0      0   0    1\n",
       "3    0   1    1   0    1   0    0      0   1    1\n",
       "4    0   0    0   0    0   0    0      1   0    0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categorical_most_frequent.replace(no_map,inplace=True)\n",
    "categorical_most_frequent.replace(norm_map,inplace=True)\n",
    "categorical_most_frequent.replace(pres_map,inplace=True)\n",
    "categorical_most_frequent.replace(good_map,inplace=True)\n",
    "categorical_most_frequent.dm.replace(\" yes\",1,inplace=True)\n",
    "categorical_most_frequent.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. for the numeric data\n",
    "median_per_column = kidney_data[kidney_columns[:14]].apply(lambda x: x.median(),axis=0)\n",
    "numeric_median_filled = kidney_data[kidney_columns[:14]].fillna(median_per_column,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler() #create a scaler object\n",
    "scaler.fit(numeric_median_filled) #fit the scaler\n",
    "numeric_scaled = scaler.transform(numeric_median_filled) #transform the data with it\n",
    "numeric_scaled = pd.DataFrame(numeric_scaled,columns=kidney_columns[:14])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_kidney_data = pd.concat((numeric_scaled,categorical_most_frequent),axis=1)\n",
    "cleaned_kidney_data[\"target\"] = (kidney_data[\"class\"]==\"ckd\").astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets take a look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>bp</th>\n",
       "      <th>sg</th>\n",
       "      <th>al</th>\n",
       "      <th>su</th>\n",
       "      <th>bgr</th>\n",
       "      <th>bu</th>\n",
       "      <th>sc</th>\n",
       "      <th>sod</th>\n",
       "      <th>pot</th>\n",
       "      <th>...</th>\n",
       "      <th>pc</th>\n",
       "      <th>pcc</th>\n",
       "      <th>ba</th>\n",
       "      <th>htn</th>\n",
       "      <th>dm</th>\n",
       "      <th>cad</th>\n",
       "      <th>appet</th>\n",
       "      <th>pe</th>\n",
       "      <th>ane</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.210031</td>\n",
       "      <td>0.254214</td>\n",
       "      <td>0.421486</td>\n",
       "      <td>0.076249</td>\n",
       "      <td>-0.380269</td>\n",
       "      <td>-0.320122</td>\n",
       "      <td>-0.419451</td>\n",
       "      <td>-0.319668</td>\n",
       "      <td>0.040104</td>\n",
       "      <td>-0.062903</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-2.627234</td>\n",
       "      <td>-1.972476</td>\n",
       "      <td>0.421486</td>\n",
       "      <td>2.363728</td>\n",
       "      <td>-0.380269</td>\n",
       "      <td>-0.320122</td>\n",
       "      <td>-0.784315</td>\n",
       "      <td>-0.390819</td>\n",
       "      <td>0.040104</td>\n",
       "      <td>-0.062903</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.615355</td>\n",
       "      <td>0.254214</td>\n",
       "      <td>-1.421074</td>\n",
       "      <td>0.838742</td>\n",
       "      <td>2.507853</td>\n",
       "      <td>3.697618</td>\n",
       "      <td>-0.074858</td>\n",
       "      <td>-0.212942</td>\n",
       "      <td>0.040104</td>\n",
       "      <td>-0.062903</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.210031</td>\n",
       "      <td>-0.488016</td>\n",
       "      <td>-2.342354</td>\n",
       "      <td>2.363728</td>\n",
       "      <td>-0.380269</td>\n",
       "      <td>-0.373337</td>\n",
       "      <td>-0.014047</td>\n",
       "      <td>0.142813</td>\n",
       "      <td>-2.896333</td>\n",
       "      <td>-0.737181</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.033163</td>\n",
       "      <td>0.254214</td>\n",
       "      <td>-1.421074</td>\n",
       "      <td>0.838742</td>\n",
       "      <td>-0.380269</td>\n",
       "      <td>-0.519679</td>\n",
       "      <td>-0.622154</td>\n",
       "      <td>-0.284093</td>\n",
       "      <td>0.040104</td>\n",
       "      <td>-0.062903</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        age        bp        sg        al        su       bgr        bu  \\\n",
       "0 -0.210031  0.254214  0.421486  0.076249 -0.380269 -0.320122 -0.419451   \n",
       "1 -2.627234 -1.972476  0.421486  2.363728 -0.380269 -0.320122 -0.784315   \n",
       "2  0.615355  0.254214 -1.421074  0.838742  2.507853  3.697618 -0.074858   \n",
       "3 -0.210031 -0.488016 -2.342354  2.363728 -0.380269 -0.373337 -0.014047   \n",
       "4 -0.033163  0.254214 -1.421074  0.838742 -0.380269 -0.519679 -0.622154   \n",
       "\n",
       "         sc       sod       pot   ...    pc  pcc  ba  htn  dm  cad  appet  pe  \\\n",
       "0 -0.319668  0.040104 -0.062903   ...     0    0   0    1   1    0      1   0   \n",
       "1 -0.390819  0.040104 -0.062903   ...     0    0   0    0   0    0      1   0   \n",
       "2 -0.212942  0.040104 -0.062903   ...     0    0   0    0   1    0      0   0   \n",
       "3  0.142813 -2.896333 -0.737181   ...     1    1   0    1   0    0      0   1   \n",
       "4 -0.284093  0.040104 -0.062903   ...     0    0   0    0   0    0      1   0   \n",
       "\n",
       "   ane  target  \n",
       "0    0       1  \n",
       "1    0       1  \n",
       "2    1       1  \n",
       "3    1       1  \n",
       "4    0       1  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_kidney_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's look at the correlation matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1a161ca400>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = sns.mpl.pyplot.subplots(figsize=(10,10))\n",
    "sns.heatmap(cleaned_kidney_data.corr(),ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how many of the features are very strongly correlated or anti-correlated with each other. High correlation among features is not ideal for building good predictive models. This will motivate our discussion of regularization.\n",
    "\n",
    "* Multicollinearity obfuscates what is attributable to the model's predictive power.\n",
    "\n",
    "Before we get into this, a brief aside on cross-validation as opposed to using a train/test split for model evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Brief aside on cross-validation\n",
    "\n",
    "There are drawbacks when just using a single train/test split.\n",
    "\n",
    "**Specifically:**\n",
    "\n",
    "Using one train/test split provides a high variance estimate since changing which observations happen to be in the testing set can significantly change testing accuracy.\n",
    "What would be a better approach?\n",
    "\n",
    "**Better approach:** Create a bunch of train/test splits, calculate the testing accuracy for each, and average the results together.\n",
    "\n",
    "**This is the essense of cross-validation.**\n",
    "![5 fold cv](../images/5-fold-cv.png)\n",
    "\n",
    "Altnernative: **leave-one sample testing**\n",
    "* when there is no obvious grouping\n",
    "* leave one sample out and do a training and do a testing on that sample that was left out; repeat until all the samples are exhuasted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y = cleaned_kidney_data.iloc[:,:-1],cleaned_kidney_data.target\n",
    "\n",
    "lr = LogisticRegression(C=1e8,random_state=1)\n",
    "\n",
    "# test-set accuracy\n",
    "cv_scores = cross_val_score(lr,X,y,\n",
    "                            cv=10,\n",
    "                            scoring='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 fold cv accuracy: 0.990\n"
     ]
    }
   ],
   "source": [
    "print(\"10 fold cv accuracy: {:0.3f}\".format(cv_scores.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretty good accuracy!\n",
    "\n",
    "Now let's see what the coefficients of a model trained on all of this data look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>features</th>\n",
       "      <th>betas</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>htn</td>\n",
       "      <td>12.581493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>dm</td>\n",
       "      <td>11.526347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>pe</td>\n",
       "      <td>10.371044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>al</td>\n",
       "      <td>7.452525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>bgr</td>\n",
       "      <td>6.624110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>bu</td>\n",
       "      <td>6.529022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>rbc</td>\n",
       "      <td>5.077182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>sc</td>\n",
       "      <td>4.187178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>pc</td>\n",
       "      <td>2.031988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>pcc</td>\n",
       "      <td>1.550015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>sod</td>\n",
       "      <td>1.457274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>su</td>\n",
       "      <td>1.148232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bp</td>\n",
       "      <td>0.789970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>ane</td>\n",
       "      <td>0.568395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>ba</td>\n",
       "      <td>0.454597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>cad</td>\n",
       "      <td>-0.098013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>age</td>\n",
       "      <td>-1.490904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>rc</td>\n",
       "      <td>-1.937088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>wc</td>\n",
       "      <td>-3.909674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>pcv</td>\n",
       "      <td>-7.800954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>pot</td>\n",
       "      <td>-8.450024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>hemo</td>\n",
       "      <td>-12.756174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>appet</td>\n",
       "      <td>-13.633558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sg</td>\n",
       "      <td>-15.075376</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   features      betas\n",
       "18      htn  12.581493\n",
       "19       dm  11.526347\n",
       "22       pe  10.371044\n",
       "3        al   7.452525\n",
       "5       bgr   6.624110\n",
       "6        bu   6.529022\n",
       "14      rbc   5.077182\n",
       "7        sc   4.187178\n",
       "15       pc   2.031988\n",
       "16      pcc   1.550015\n",
       "8       sod   1.457274\n",
       "4        su   1.148232\n",
       "1        bp   0.789970\n",
       "23      ane   0.568395\n",
       "17       ba   0.454597\n",
       "20      cad  -0.098013\n",
       "0       age  -1.490904\n",
       "13       rc  -1.937088\n",
       "12       wc  -3.909674\n",
       "11      pcv  -7.800954\n",
       "9       pot  -8.450024\n",
       "10     hemo -12.756174\n",
       "21    appet -13.633558\n",
       "2        sg -15.075376"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# refitting the logistic model on the entire test set\n",
    "coeffs = LogisticRegression(C=1e8).fit(X,y).coef_\n",
    "\n",
    "# data wrangling\n",
    "coeff_df = pd.DataFrame(list(zip(cleaned_kidney_data.columns.tolist()[:-1],coeffs[0])),columns=[\"features\",\"betas\"])\n",
    "coeff_df.sort_values(\"betas\",ascending=False,inplace=True)\n",
    "coeff_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's try something else with this dataset to see if we can improve the overall predictive performance of our model.\n",
    "\n",
    "Let's generate additional polynomial features up to degree 2 (all pairwise interactions among our original columns), and see if we can't improve the overall RMSE (by adding non-linearities that might be predictive apart from the linear features we've had so far):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(400, 300)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#generate the transformer\n",
    "pf_2 = PolynomialFeatures(degree=2,\n",
    "                          interaction_only=True, # this excludes the polynomial of the terms (e.g., a^2, b^2)\n",
    "                          include_bias=False) # ignore constant column\n",
    "\n",
    "#apply it to the data, but ignore the first constant column\n",
    "pf_2_data = pf_2.fit_transform(X)\n",
    "print(pf_2_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10-fold mean RMSE for degree-2 case:  0.9599999999999997\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression(C=1e8,random_state=1)\n",
    "#get the mean squared errors per fold, 10 fold cv\n",
    "errors = cross_val_score(lr,pf_2_data,cleaned_kidney_data.target,cv=10,scoring=\"accuracy\")\n",
    "#convert each fold's mean squared errors to rmse\n",
    "print(\"10-fold mean RMSE for degree-2 case: \",np.mean(errors))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So that model performed worse when we gave it non-linear features. Why would that happen? 2 reasons:\n",
    "\n",
    "1. We are injection new columns that are highly correlated with our original columns, along with nonlinear information.\n",
    "2. At this point, the number of columns in our dataset is very close to the number of samples we have. This is dangerously close to an over-specified model (too many unknowns relative to number of data points).\n",
    "\n",
    "We generated interaction features and our accuracy went down marginally. How can we integrate the additional information from interaction terms, limit the impact of highly correlated features, and attempt to boost our model's performance?\n",
    "\n",
    "**We can apply what is called REGULARIZATION!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regularization is a method for penalizing overly complicated models, while minimizing out-of-sample (test-set) error. We will talk about 2 kinds of regularization, which have different names, depending on whether they are being used for regression or classification:\n",
    "\n",
    " * **Lasso/L1 Regularization**\n",
    " * **Ridge/L2 Regularization**\n",
    "\n",
    "In essence, regularization is a method for adding additional constraints or penalties to a model, with the goal of preventing overfitting and improving generalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lasso Regression / L1 Regularization for Logistic Regression\n",
    "\n",
    "In traditional regression/classification, we try to minimize some kind of loss function (we try to make the loss be as close to 0 as possible). In regression, this is usually the **mean squared error** (or root mean squared error). However, with regularization, instead of just minimizing the loss based on the error of the model, we can also add a term that effectively \"punishes\" more complicated models.\n",
    "\n",
    "What this means is that as you add more terms (features/columns) to your model, you suffer more added \"loss due to complexity\" in addition to \"loss due to model performance.\" \n",
    "\n",
    "So, how can regularization be used in linear regression to effectively make models simpler?\n",
    "\n",
    "**You reduce the coefficients for specific columns in your model until they go to zero (or very close to zero) for your model, which has the net effect of not using those features (or using them very sparingly)**\n",
    "\n",
    "More mathematically, Lasso/L1 regularization adds an absolute-value penalty to the loss function:\n",
    "\n",
    "$$loss_{lasso} = |Xw-Y|^2 + \\lambda|w|$$\n",
    "\n",
    "*lamda*: controls the strength of the regularization; varies between 0 - 1; serves as another parameter. \n",
    "*w*: magnitude of the coefficients. \n",
    "\n",
    "Whereas the non-regularized version is simply:\n",
    "\n",
    "$$loss = |Xw-Y|^2$$\n",
    "\n",
    "So, L1 regularization adds a penalty (based on $\\lambda$) to the regular loss function. Since each non-zero coefficient adds to the penalty, it forces weak features to have zero as coefficients. Thus L1 regularization produces sparse solutions, inherently performing feature selection.\n",
    "\n",
    "However, because this is an absolute value-based regularization (and absolute values are no-fun when it comes to taking derivatives), L1/Lasso regularization can lead to non-unique solutions (especially when you have highly correlated features...why?).\n",
    "\n",
    "For classification, Scikit-learn requires that we use the `LogisticRegression` model with an additional parameter `penalty=l1`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1_all = LogisticRegression(penalty=\"l1\",#increase the number of iterations for lasso to prevent lack of convergence\n",
    "                            C=1) # C = 1 using regulariazation; C = 1e8 is not using regularization \n",
    "\n",
    "accuracies_poly2_lasso = cross_val_score(l1_all,\n",
    "                                         pf_2_data,\n",
    "                                         cleaned_kidney_data.target,\n",
    "                                         cv=10,\n",
    "                                         scoring=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10-fold accuracy for degree-2 case, lasso regularization: 0.990\n"
     ]
    }
   ],
   "source": [
    "print(\"10-fold accuracy for degree-2 case, lasso regularization: {:0.03f}\".format(np.mean(accuracies_poly2_lasso)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That worked better than the full degree 2 model, but not better than the original.\n",
    "\n",
    "Let's examine how many of the coefficients are non-zero when we fit on the full dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features in the model:  300\n",
      "[[False  True  True  True False  True False  True  True False  True False\n",
      "  False  True False False False False  True  True False False  True False\n",
      "   True False False False  True False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False  True False False False False False\n",
      "  False False False False False  True False False False False False False\n",
      "  False False False  True False False False False False  True False False\n",
      "  False  True False  True False False False False False False False  True\n",
      "  False False False False False False False False  True False False False\n",
      "  False False False False False False  True False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "   True False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False  True False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False  True False False False False False False False False\n",
      "  False False False  True False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "   True False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False]]\n",
      "\n",
      "\n",
      "Number of non-zero features in the model:  27\n",
      "Fraction of total features used:  0.09\n"
     ]
    }
   ],
   "source": [
    "lasso_all = LogisticRegression(C=1, penalty=\"l1\").fit(pf_2_data,cleaned_kidney_data.target)\n",
    "print(\"Number of features in the model: \",len(lasso_all.coef_[0]))\n",
    "non_zero_features_mask = np.abs(lasso_all.coef_)>0.0001\n",
    "print(non_zero_features_mask)\n",
    "print(\"\\n\")\n",
    "print(\"Number of non-zero features in the model: \",np.sum(non_zero_features_mask.astype(int)))\n",
    "print(\"Fraction of total features used: \",float(np.sum(non_zero_features_mask.astype(int)))/len(lasso_all.coef_[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1a21e3aa20>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.distplot(lasso_all.coef_[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, with full lasso regularization, we get a model that uses less than 30% of the 300 polynomial columns. A better example here would be to compare the coefficients of the original dataset with and without regularization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y = cleaned_kidney_data.iloc[:,:-1],cleaned_kidney_data.target\n",
    "\n",
    "lr = LogisticRegression(C=1e8)\n",
    "lr_l1 = LogisticRegression(C=1,penalty=\"l1\")\n",
    "cv_scores_nol1 = cross_val_score(lr,X,y,cv=10,scoring='accuracy')\n",
    "cv_scores_l1 = cross_val_score(lr_l1,X,y,cv=10,scoring='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full LR model, no regularization: 0.990\n",
      "L1 penalized LR model, strong regularization: 0.982\n"
     ]
    }
   ],
   "source": [
    "print(\"Full LR model, no regularization: {:0.3f}\".format(np.mean(cv_scores_nol1)))\n",
    "print(\"L1 penalized LR model, strong regularization: {:0.3f}\".format(np.mean(cv_scores_l1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now compare the coefficients of both models, trained on all of the available data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>features</th>\n",
       "      <th>betas_noL1</th>\n",
       "      <th>betas_L1</th>\n",
       "      <th>beta_diffs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>age</td>\n",
       "      <td>-1.490904</td>\n",
       "      <td>-0.008730</td>\n",
       "      <td>1.482174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bp</td>\n",
       "      <td>0.789970</td>\n",
       "      <td>0.169060</td>\n",
       "      <td>0.620910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sg</td>\n",
       "      <td>-15.075376</td>\n",
       "      <td>-2.483659</td>\n",
       "      <td>12.591717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>al</td>\n",
       "      <td>7.452525</td>\n",
       "      <td>1.497404</td>\n",
       "      <td>5.955121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>su</td>\n",
       "      <td>1.148232</td>\n",
       "      <td>0.469512</td>\n",
       "      <td>0.678720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>bgr</td>\n",
       "      <td>6.624110</td>\n",
       "      <td>0.372465</td>\n",
       "      <td>6.251644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>bu</td>\n",
       "      <td>6.529022</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.529022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>sc</td>\n",
       "      <td>4.187178</td>\n",
       "      <td>0.670444</td>\n",
       "      <td>3.516734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>sod</td>\n",
       "      <td>1.457274</td>\n",
       "      <td>-0.304417</td>\n",
       "      <td>1.152857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>pot</td>\n",
       "      <td>-8.450024</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.450024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>hemo</td>\n",
       "      <td>-12.756174</td>\n",
       "      <td>-2.559246</td>\n",
       "      <td>10.196927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>pcv</td>\n",
       "      <td>-7.800954</td>\n",
       "      <td>-1.064974</td>\n",
       "      <td>6.735980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>wc</td>\n",
       "      <td>-3.909674</td>\n",
       "      <td>-0.092997</td>\n",
       "      <td>3.816677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>rc</td>\n",
       "      <td>-1.937088</td>\n",
       "      <td>-0.549983</td>\n",
       "      <td>1.387106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>rbc</td>\n",
       "      <td>5.077182</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.077182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>pc</td>\n",
       "      <td>2.031988</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.031988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>pcc</td>\n",
       "      <td>1.550015</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.550015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>ba</td>\n",
       "      <td>0.454597</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.454597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>htn</td>\n",
       "      <td>12.581493</td>\n",
       "      <td>1.541916</td>\n",
       "      <td>11.039577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>dm</td>\n",
       "      <td>11.526347</td>\n",
       "      <td>1.949814</td>\n",
       "      <td>9.576532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>cad</td>\n",
       "      <td>-0.098013</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.098013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>appet</td>\n",
       "      <td>-13.633558</td>\n",
       "      <td>-0.149486</td>\n",
       "      <td>13.484072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>pe</td>\n",
       "      <td>10.371044</td>\n",
       "      <td>1.409946</td>\n",
       "      <td>8.961097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>ane</td>\n",
       "      <td>0.568395</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.568395</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   features  betas_noL1  betas_L1  beta_diffs\n",
       "0       age   -1.490904 -0.008730    1.482174\n",
       "1        bp    0.789970  0.169060    0.620910\n",
       "2        sg  -15.075376 -2.483659   12.591717\n",
       "3        al    7.452525  1.497404    5.955121\n",
       "4        su    1.148232  0.469512    0.678720\n",
       "5       bgr    6.624110  0.372465    6.251644\n",
       "6        bu    6.529022  0.000000    6.529022\n",
       "7        sc    4.187178  0.670444    3.516734\n",
       "8       sod    1.457274 -0.304417    1.152857\n",
       "9       pot   -8.450024  0.000000    8.450024\n",
       "10     hemo  -12.756174 -2.559246   10.196927\n",
       "11      pcv   -7.800954 -1.064974    6.735980\n",
       "12       wc   -3.909674 -0.092997    3.816677\n",
       "13       rc   -1.937088 -0.549983    1.387106\n",
       "14      rbc    5.077182  0.000000    5.077182\n",
       "15       pc    2.031988  0.000000    2.031988\n",
       "16      pcc    1.550015  0.000000    1.550015\n",
       "17       ba    0.454597  0.000000    0.454597\n",
       "18      htn   12.581493  1.541916   11.039577\n",
       "19       dm   11.526347  1.949814    9.576532\n",
       "20      cad   -0.098013  0.000000    0.098013\n",
       "21    appet  -13.633558 -0.149486   13.484072\n",
       "22       pe   10.371044  1.409946    8.961097\n",
       "23      ane    0.568395  0.000000    0.568395"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X,y = cleaned_kidney_data.iloc[:,:-1],cleaned_kidney_data.target\n",
    "\n",
    "coeffs_noreg = LogisticRegression(C=1e8).fit(X,y).coef_[0]\n",
    "coeffs_l1reg = LogisticRegression(penalty=\"l1\",C=1).fit(X,y).coef_[0]\n",
    "coeff_df = pd.DataFrame( list(zip(cleaned_kidney_data.columns.tolist()[:-1],\n",
    "                                  coeffs_noreg,coeffs_l1reg)),\n",
    "                        columns=[\"features\",\"betas_noL1\",\"betas_L1\"])\n",
    "coeff_df[\"beta_diffs\"] = np.abs(coeff_df.betas_noL1) - np.abs(coeff_df.betas_L1)\n",
    "coeff_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that:\n",
    "\n",
    "* Many coefficients are now effectively 0 (this means the column is not being used in the prediction)\n",
    "* For those coefficients that still are being used, many of their magnitudes have gotten smaller\n",
    "* Although we are using significantly fewer coefficients, our model's performance has not significantly suffered. We have effectively gotten a \"simpler\" model for relatively little cost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise Time!\n",
    "\n",
    " * Try the l1 regularization method but vary the `C` parameter. Inspect the coefficients and the cv-accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge Regression / L2 Regularization for Logistic Regression\n",
    "\n",
    "So, Lasso looks really excellent to use a regularization method. However, as I'd mentioned before, it is difficult to use across the board for all regression/classification problems, because it can create non-unique solutions, or solutions don't always converge (because of the problem with taking derivatives of a function that is not differentiable everywhere).\n",
    "\n",
    "So, what can be done? We can create a penalty term that is smooth and therefore differentiable everywhere. Specifically, we can use what is called **Ridge/L2 regularization.** Here, the loss is slightly different:\n",
    "\n",
    "$$loss_{ridge} = |Xw-Y|^2 + \\lambda|w|^2$$\n",
    "\n",
    "The act of squaring the weight features makes the penalty differentiable (because the values now smoothly transition to 0). So, squaring the L1 penalty turns it into an L2 penalty. **Square the Lasso and get a Ridge (or something).**\n",
    "\n",
    "Since the coefficients are squared in the penalty expression, it has a different effect, namely it forces the coefficient values to be spread out more equally. For correlated features, it means that they tend to get similar coefficients (whereas in L1 regularization, one of the terms will usually be forced to 0).\n",
    "\n",
    "**The effect of this is that models are much more stable (coefficients do not fluctuate on small data changes as is the case with unregularized or L1 models). So while Ridge/L2 regularization does not perform feature selection the same way as L1 does, it is much more useful for feature *interpretation*; a predictive feature will get a non-zero coefficient, which is often not the case with L1.**\n",
    "\n",
    "Let's now use the full-penalty Ridge regression and see what happens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10-fold mean accuracy, strong ridge regularization:  0.9949999999999999\n"
     ]
    }
   ],
   "source": [
    "ridge = LogisticRegression(penalty=\"l2\",C=1)\n",
    "ridge_cv_accuracies = cross_val_score(ridge,\n",
    "                                      cleaned_kidney_data.iloc[:,:-1],\n",
    "                                      cleaned_kidney_data.target,\n",
    "                                      cv=10,\n",
    "                                      scoring=\"accuracy\")\n",
    "print(\"10-fold mean accuracy, strong ridge regularization: \",np.mean(ridge_cv_accuracies))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets compare the accuracies of the 3 models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(C=1e9)\n",
    "l1 = LogisticRegression(penalty=\"l1\",C=1)\n",
    "l2 = LogisticRegression(penalty=\"l2\",C=1)\n",
    "\n",
    "lr_accs = cross_val_score(lr,\n",
    "                          cleaned_kidney_data.iloc[:,:-1],\n",
    "                          cleaned_kidney_data.target,\n",
    "                          cv=10,\n",
    "                          scoring=\"accuracy\")\n",
    "l1_accs = cross_val_score(l1,\n",
    "                          cleaned_kidney_data.iloc[:,:-1],\n",
    "                          cleaned_kidney_data.target,\n",
    "                          cv=10,\n",
    "                          scoring=\"accuracy\")\n",
    "l2_accs = cross_val_score(l2,\n",
    "                          cleaned_kidney_data.iloc[:,:-1],\n",
    "                          cleaned_kidney_data.target,\n",
    "                          cv=10,\n",
    "                          scoring=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.99\n",
      "0.985\n",
      "0.9949999999999999\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(lr_accs)) # original model\n",
    "print(np.mean(l1_accs)) # penalizing smaller terms (L1 regularization)\n",
    "print(np.mean(l2_accs)) # smoothing (L2 regulazation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compare the fitted coefficients for each of the 3 models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>features</th>\n",
       "      <th>betas</th>\n",
       "      <th>betas_L1</th>\n",
       "      <th>betas_L2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>age</td>\n",
       "      <td>-1.490904</td>\n",
       "      <td>-0.008182</td>\n",
       "      <td>-0.230692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bp</td>\n",
       "      <td>0.789970</td>\n",
       "      <td>0.168701</td>\n",
       "      <td>0.343276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sg</td>\n",
       "      <td>-15.075376</td>\n",
       "      <td>-2.484992</td>\n",
       "      <td>-1.897266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>al</td>\n",
       "      <td>7.452525</td>\n",
       "      <td>1.495728</td>\n",
       "      <td>1.096099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>su</td>\n",
       "      <td>1.148232</td>\n",
       "      <td>0.469074</td>\n",
       "      <td>0.593027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>bgr</td>\n",
       "      <td>6.624110</td>\n",
       "      <td>0.372945</td>\n",
       "      <td>0.542160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>bu</td>\n",
       "      <td>6.529022</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.157599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>sc</td>\n",
       "      <td>4.187178</td>\n",
       "      <td>0.668117</td>\n",
       "      <td>0.882907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>sod</td>\n",
       "      <td>1.457274</td>\n",
       "      <td>-0.305794</td>\n",
       "      <td>-0.545554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>pot</td>\n",
       "      <td>-8.450024</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.267640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>hemo</td>\n",
       "      <td>-12.756174</td>\n",
       "      <td>-2.556539</td>\n",
       "      <td>-1.739368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>pcv</td>\n",
       "      <td>-7.800954</td>\n",
       "      <td>-1.063283</td>\n",
       "      <td>-1.072261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>wc</td>\n",
       "      <td>-3.909674</td>\n",
       "      <td>-0.092055</td>\n",
       "      <td>-0.161704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>rc</td>\n",
       "      <td>-1.937088</td>\n",
       "      <td>-0.551134</td>\n",
       "      <td>-0.727677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>rbc</td>\n",
       "      <td>5.077182</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.665567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>pc</td>\n",
       "      <td>2.031988</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.509287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>pcc</td>\n",
       "      <td>1.550015</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.110940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>ba</td>\n",
       "      <td>0.454597</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.092490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>htn</td>\n",
       "      <td>12.581493</td>\n",
       "      <td>1.540650</td>\n",
       "      <td>1.502515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>dm</td>\n",
       "      <td>11.526347</td>\n",
       "      <td>1.947981</td>\n",
       "      <td>1.461289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>cad</td>\n",
       "      <td>-0.098013</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.057659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>appet</td>\n",
       "      <td>-13.633558</td>\n",
       "      <td>-0.152119</td>\n",
       "      <td>0.002702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>pe</td>\n",
       "      <td>10.371044</td>\n",
       "      <td>1.408375</td>\n",
       "      <td>1.163218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>ane</td>\n",
       "      <td>0.568395</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.440307</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   features      betas  betas_L1  betas_L2\n",
       "0       age  -1.490904 -0.008182 -0.230692\n",
       "1        bp   0.789970  0.168701  0.343276\n",
       "2        sg -15.075376 -2.484992 -1.897266\n",
       "3        al   7.452525  1.495728  1.096099\n",
       "4        su   1.148232  0.469074  0.593027\n",
       "5       bgr   6.624110  0.372945  0.542160\n",
       "6        bu   6.529022  0.000000  0.157599\n",
       "7        sc   4.187178  0.668117  0.882907\n",
       "8       sod   1.457274 -0.305794 -0.545554\n",
       "9       pot  -8.450024  0.000000 -0.267640\n",
       "10     hemo -12.756174 -2.556539 -1.739368\n",
       "11      pcv  -7.800954 -1.063283 -1.072261\n",
       "12       wc  -3.909674 -0.092055 -0.161704\n",
       "13       rc  -1.937088 -0.551134 -0.727677\n",
       "14      rbc   5.077182  0.000000  0.665567\n",
       "15       pc   2.031988  0.000000  0.509287\n",
       "16      pcc   1.550015  0.000000  0.110940\n",
       "17       ba   0.454597  0.000000  0.092490\n",
       "18      htn  12.581493  1.540650  1.502515\n",
       "19       dm  11.526347  1.947981  1.461289\n",
       "20      cad  -0.098013  0.000000  0.057659\n",
       "21    appet -13.633558 -0.152119  0.002702\n",
       "22       pe  10.371044  1.408375  1.163218\n",
       "23      ane   0.568395  0.000000  0.440307"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X,y = cleaned_kidney_data.iloc[:,:-1],cleaned_kidney_data.target\n",
    "\n",
    "coeffs_noreg = LogisticRegression(C=1e8).fit(X,y).coef_[0]\n",
    "coeffs_l1reg = LogisticRegression(penalty=\"l1\",C=1).fit(X,y).coef_[0]\n",
    "coeffs_l2reg = LogisticRegression(penalty=\"l2\",C=1).fit(X,y).coef_[0]\n",
    "coeff_df = pd.DataFrame(list(zip(cleaned_kidney_data.columns.tolist()[:-1],\n",
    "                                 coeffs_noreg,\n",
    "                                 coeffs_l1reg, \n",
    "                                 coeffs_l2reg)),\n",
    "                        columns=[\"features\",\"betas\",\"betas_L1\",\"betas_L2\"])\n",
    "coeff_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, keep in mind that you can alter the amount of regularization ($\\lambda$) you want by altering the `C` parameter for the LR model.\n",
    "\n",
    "**The higher the `C` the lower the regularization (this is why I always made the `C` parameter obscenely high before, so that the `Logistic Regression` was always un-regularized.**\n",
    "\n",
    "You can combine both `L1` and `L2` penalties together into another classifer called an `ElasticNet`. Furthermore, you can use sklearn to automatically find an ideal regularization value for you using either the `ElasticNetCV` or `LogisticRegressionCV` classes.\n",
    "\n",
    "However, if you want to use the `ElasticNet` classifier inside of other scikit learn functions, you must use the `SGDClassifier` class instead:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ElasticNetCV(alphas=None, copy_X=True, cv=10, eps=0.001, fit_intercept=True,\n",
       "       l1_ratio=array([0.01, 0.06, 0.11, 0.16, 0.21, 0.26, 0.31, 0.36, 0.41, 0.46, 0.51,\n",
       "       0.56, 0.61, 0.66, 0.71, 0.76, 0.81, 0.86, 0.91, 0.96]),\n",
       "       max_iter=1000, n_alphas=100, n_jobs=1, normalize=False,\n",
       "       positive=False, precompute='auto', random_state=None,\n",
       "       selection='cyclic', tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import ElasticNetCV, LogisticRegressionCV, SGDClassifier\n",
    "\n",
    "l1_ratios = np.arange(0.01,1,.05)\n",
    "enet = ElasticNetCV(cv=10,l1_ratio=l1_ratios)\n",
    "enet.fit(cleaned_kidney_data.iloc[:,:-1],cleaned_kidney_data.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.00686385860772757"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enet.alpha_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9600000000000001"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enet.l1_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.        ,  0.02382838, -0.15455025,  0.04316651,  0.        ,\n",
       "        0.01981542, -0.02404562,  0.00886937, -0.01777904,  0.        ,\n",
       "       -0.1759993 , -0.00629706,  0.        , -0.00096261,  0.        ,\n",
       "       -0.        , -0.        , -0.        ,  0.09784247,  0.11797168,\n",
       "       -0.        , -0.03497379,  0.        , -0.        ])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enet.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "enet_coeff_df = pd.DataFrame(list(zip(cleaned_kidney_data.columns.tolist()[:-1],\n",
    "                                      enet.coef_,np.abs(enet.coef_))),\n",
    "                             columns=[\"features\",\"betas_enet\",\"abs_betas\"])\n",
    "enet_coeff_df.sort_values(\"abs_betas\",inplace=True,ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>features</th>\n",
       "      <th>betas_enet</th>\n",
       "      <th>abs_betas</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>hemo</td>\n",
       "      <td>-0.175999</td>\n",
       "      <td>0.175999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sg</td>\n",
       "      <td>-0.154550</td>\n",
       "      <td>0.154550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>dm</td>\n",
       "      <td>0.117972</td>\n",
       "      <td>0.117972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>htn</td>\n",
       "      <td>0.097842</td>\n",
       "      <td>0.097842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>al</td>\n",
       "      <td>0.043167</td>\n",
       "      <td>0.043167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>appet</td>\n",
       "      <td>-0.034974</td>\n",
       "      <td>0.034974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>bu</td>\n",
       "      <td>-0.024046</td>\n",
       "      <td>0.024046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bp</td>\n",
       "      <td>0.023828</td>\n",
       "      <td>0.023828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>bgr</td>\n",
       "      <td>0.019815</td>\n",
       "      <td>0.019815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>sod</td>\n",
       "      <td>-0.017779</td>\n",
       "      <td>0.017779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>sc</td>\n",
       "      <td>0.008869</td>\n",
       "      <td>0.008869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>pcv</td>\n",
       "      <td>-0.006297</td>\n",
       "      <td>0.006297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>rc</td>\n",
       "      <td>-0.000963</td>\n",
       "      <td>0.000963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>ba</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>cad</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>pe</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>age</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>wc</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>pcc</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>pc</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>rbc</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>pot</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>su</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>ane</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   features  betas_enet  abs_betas\n",
       "10     hemo   -0.175999   0.175999\n",
       "2        sg   -0.154550   0.154550\n",
       "19       dm    0.117972   0.117972\n",
       "18      htn    0.097842   0.097842\n",
       "3        al    0.043167   0.043167\n",
       "21    appet   -0.034974   0.034974\n",
       "6        bu   -0.024046   0.024046\n",
       "1        bp    0.023828   0.023828\n",
       "5       bgr    0.019815   0.019815\n",
       "8       sod   -0.017779   0.017779\n",
       "7        sc    0.008869   0.008869\n",
       "11      pcv   -0.006297   0.006297\n",
       "13       rc   -0.000963   0.000963\n",
       "17       ba   -0.000000   0.000000\n",
       "20      cad   -0.000000   0.000000\n",
       "22       pe    0.000000   0.000000\n",
       "0       age    0.000000   0.000000\n",
       "12       wc    0.000000   0.000000\n",
       "16      pcc   -0.000000   0.000000\n",
       "15       pc   -0.000000   0.000000\n",
       "14      rbc    0.000000   0.000000\n",
       "9       pot    0.000000   0.000000\n",
       "4        su    0.000000   0.000000\n",
       "23      ane   -0.000000   0.000000"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enet_coeff_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9775\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:131: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "/usr/local/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:131: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "/usr/local/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:131: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "/usr/local/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:131: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "/usr/local/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:131: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "/usr/local/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:131: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "/usr/local/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:131: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "/usr/local/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:131: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "/usr/local/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:131: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "/usr/local/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:131: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "best_enet = SGDClassifier(alpha=enet.alpha_,loss=\"log\",penalty=\"elasticnet\",l1_ratio=enet.l1_ratio_)\n",
    "\n",
    "enet_accs = cross_val_score(best_enet,\n",
    "                          cleaned_kidney_data.iloc[:,:-1],\n",
    "                          cleaned_kidney_data.target,\n",
    "                          cv=10,\n",
    "                          scoring=\"accuracy\")\n",
    "print(np.mean(enet_accs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise Time\n",
    "\n",
    "* Use the `elastic net` regressor on the data and see if you can't find an ideal cross-validated combination of alpha and l1-ratio value for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lasso vs. Ridge\n",
    "\n",
    "#### Lasso:\n",
    "* produces sparse models \n",
    "* is useful for strong feature selection in order to improve model performance, or to minimize the number of explanatory variables.\n",
    "* can produce non-unique solutions (when some features are very strongly correlated)\n",
    "* can produce very different solutions depending on slight changes in features (because of non-uniqueness)\n",
    "\n",
    "#### Ridge:\n",
    "* produces stable models with smooth non-zero coefficients across features. \n",
    "* is useful for data interpretation, understanding what features, even when correlated, may be used in combination to predict the response.\n",
    "* may tell you something about how the data itself was generated.\n",
    "\n",
    "You can combine both Lasso and Ridge models into a single penalized model (that uses a weighted combination of Lasso and Ridge regression). This is called the `ElasticNet`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### General Strategy and Takeaways\n",
    "\n",
    "* If you are trying for data explanation, use Ridge/L2 regularization\n",
    "* If you are going for sparse models, use Lasso/L1, but realize that the solution may be non-unique. In general, Lasso doesn't always perform better than Ridge.\n",
    "* `ElasticNet` requires significantly more tuning than either method, but can lead to the highest performing \"linear\" models.\n",
    "\n",
    "**As always, with great power comes great responsibility, so use all of these methods wisely.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
